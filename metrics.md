# Метрики проекта

В этом документе описаны различные метрики, которые могут быть использованы для оценки качества работы систем распознавания и обработки речи.

## Таблица метрик

### Метрики для оценки распознавания речи и обработки текста

| Название | Описание на русском | Описание на английском | Ссылка |
|----------|---------------------|------------------------|--------|
| WER (Word Error Rate) | Коэффициент словесных ошибок — метрика, измеряющая количество ошибок при распознавании речи на уровне слов. Вычисляется как отношение суммы числа замен (S), удалений (D) и вставок (I) к общему количеству слов в эталонной фразе (N): WER = (S + D + I) / N. Чем ниже значение, тем лучше работает система распознавания речи, при этом WER = 0 считается идеальным результатом. | Word error rate (WER) is a common metric of the performance of an automatic speech recognition system. It measures the number of errors at the word level. WER is calculated as the ratio of the sum of substitutions (S), deletions (D), and insertions (I) to the total number of words in the reference (N): WER = (S + D + I) / N. The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score. | [Wikipedia: Word Error Rate](https://en.wikipedia.org/wiki/Word_error_rate) |
| CER (Character Error Rate) | Коэффициент символьных ошибок — метрика, аналогичная WER, но работающая на уровне символов вместо слов. Вычисляется как отношение суммы числа замен (S), удалений (D) и вставок (I) символов к общему количеству символов в эталонном тексте (N): CER = (S + D + I) / N. Значение CER не всегда находится между 0 и 1, особенно при большом количестве вставок. Чем ниже значение, тем лучше работает система распознавания речи. | Character error rate (CER) is similar to Word Error Rate (WER), but operates on characters instead of words. It is calculated as: CER = (S + D + I) / N, where S is the number of substitutions, D is the number of deletions, I is the number of insertions, and N is the number of characters in the reference. CER's output is not always a number between 0 and 1, particularly when there is a high number of insertions. The lower the value, the better the performance of the ASR system. | [HuggingFace: Character Error Rate](https://huggingface.co/metrics/cer) |
| Расстояние Джаро–Винклера (Jaro–Winkler distance) | Метрика сходства строк, которая является вариантом расстояния Джаро, дающая более высокие рейтинги строкам, которые совпадают с самого начала. Часто используется для сравнения коротких строк, таких как имена и фамилии. Значение метрики находится в диапазоне от 0 до 1, где 1 означает точное совпадение, а 0 — полное несовпадение. Чем ближе значение к 1, тем более похожи строки. | The Jaro–Winkler distance is a string metric measuring the edit distance between two strings. It is a variant of the Jaro distance that gives higher ratings to strings that match from the beginning. It is commonly used for comparing short strings such as names. The score is normalized such that 1 means an exact match and 0 means no similarity. The higher the score, the more similar the strings are. | [Wikipedia: Jaro–Winkler distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance), [Оценка моделей распознавания речи](https://www.assemblyai.com/blog/how-to-evaluate-speech-recognition-models) |
| SeMaScore | Новая метрика оценки систем распознавания речи, которая объединяет традиционные метрики на основе подсчета ошибок (например, WER) с оценкой семантического сходства. Особенно полезна для оценки качества распознавания речи с дефектами, шумной или акцентированной речи, где метрика WER может показывать высокие значения. Вместо того, чтобы просто подсчитывать ошибки, SeMaScore оценивает, насколько точно сгенерированная гипотеза передает смысл исходного высказывания. | SeMaScore is a new evaluation metric for automatic speech recognition (ASR) tasks. It integrates traditional error rate-based metrics (such as WER) with a robust semantic similarity-based approach. This metric is particularly useful when assessing ASR performance on disordered, noisy, or accented speech, where WER might be high. Rather than merely quantifying errors, SeMaScore evaluates how accurately the generated hypothesis conveys the intended meaning of the spoken sentence. It demonstrates robustness in handling transcription errors and strong alignment with expert human assessments. | [SeMaScore: A New Evaluation Metric for ASR Tasks](https://arxiv.org/html/2401.07506v1) |
| BLEU (Bilingual Evaluation Understudy) | Стандартная метрика для оценки качества машинного перевода. Оценивает совпадение n-грамм (последовательностей из n слов) между переводом, сделанным машиной, и эталонными переводами, сделанными людьми. Значение BLEU находится в диапазоне от 0 до 1 (или от 0 до 100…), где более высокие значения означают лучшие результаты. | BLEU (Bilingual Evaluation Understudy) is a standard metric for evaluating machine translation quality. It measures the correspondence between a machine's output and reference translations by counting matching n-grams (sequence of n words) between them. BLEU scores range from 0 to 1 (or 0 to 100%), with higher values indicating better translation quality. The metric has limitations in capturing semantic meaning and grammatical correctness but remains one of the most widely used automated measures for translation evaluation. | [HuggingFace: BLEU](https://huggingface.co/spaces/evaluate-metric/bleu), [Wikipedia: BLEU](https://en.wikipedia.org/wiki/BLEU) |
| ROUGE (Recall-Oriented Understudy for Gisting Evaluation) | Набор метрик для оценки автоматических систем суммаризации текста. Сравнивает генерируемые системой суммаризации с эталонными (написанными людьми). Включает несколько вариантов: ROUGE-N (оценивает перекрытие n-грамм), ROUGE-L (учитывает самую длинную общую подпоследовательность), ROUGE-S (учитывает перекрытие пар слов, находящихся на некотором расстоянии друг от друга) и другие. Чем выше значение ROUGE, тем лучше качество суммаризации. | ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics for evaluating automatic text summarization systems. It compares an automatically produced summary against reference summaries (typically human-created). ROUGE includes several variants: ROUGE-N (measures n-gram overlap), ROUGE-L (considers longest common subsequence), ROUGE-S (looks at skip-bigram co-occurrences), and others. Higher ROUGE scores indicate better summarization quality. While originally designed for summarization, ROUGE has been applied to various text generation tasks, including translation and dialogue generation. | [HuggingFace: ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge), [Wikipedia: ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) |
| METEOR (Metric for Evaluation of Translation with Explicit ORdering) | Метрика для оценки качества машинного перевода, которая учитывает точные совпадения слов, а также близкие варианты слов (стемминг) и синонимы. Работает в несколько этапов: сначала выравнивает слова между гипотезой и эталоном (включая точные совпадения, стемминг и синонимы), затем вычисляет точность и полноту, вводит штраф за нарушение порядка слов и вычисляет итоговый счет. Чем выше значение METEOR, тем лучше качество перевода. | METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a metric for evaluating machine translation quality that accounts for exact word matches as well as stemming and synonyms. It works in several stages: first aligning words between the hypothesis and reference (including exact matches, stemming, and synonyms), then calculating precision and recall, applying a penalty for word order differences, and computing a final score. METEOR addresses some of BLEU's limitations by considering word order and semantic similarity. Higher METEOR scores indicate better translation quality, and studies have shown that METEOR correlates better with human judgments than BLEU, especially for evaluating translation adequacy. | [HuggingFace: METEOR](https://huggingface.co/spaces/evaluate-metric/meteor), [Wikipedia: METEOR](https://en.wikipedia.org/wiki/METEOR) |
| BERTScore | Метрика оценки качества генерации текста, которая использует предварительно обученные контекстуализированные эмбеддинги BERT для сравнения сгенерированного текста с эталонным. Вместо простого подсчета совпадающих слов или n-грамм, BERTScore вычисляет косинусное сходство между векторными представлениями токенов, что позволяет учитывать семантическое сходство. Это позволяет лучше оценивать качество текста, даже если используются синонимы или перефразирование. | BERTScore is a text generation evaluation metric that uses pre-trained contextualized BERT embeddings to compare generated text with reference text. Instead of simply counting matching words or n-grams, BERTScore computes cosine similarity between token vector representations, allowing it to capture semantic similarity. This enables better evaluation of text quality even when synonyms or paraphrasing are used. BERTScore calculates precision, recall, and F1 scores based on these similarity measures. The metric has shown better correlation with human judgments than traditional metrics like BLEU or ROUGE, especially for tasks where semantic understanding is crucial. BERTScore is applicable to various text generation tasks, including translation, summarization, and caption generation. | [HuggingFace: BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore), [GitHub: BERTScore](https://github.com/Tiiiger/bert_score) |
| SacreBLEU | Стандартизированная реализация метрики BLEU, созданная для решения проблемы несовместимости разных реализаций BLEU. Разные реализации BLEU могут использовать разные правила токенизации, предобработки текста и другие настройки, что может привести к разным результатам. SacreBLEU предлагает стандартный подход к токенизации и подсчёту оценки, позволяя исследователям сравнивать результаты более объективно. | SacreBLEU is a standardized implementation of the BLEU metric, created to address inconsistencies between different BLEU implementations. Various BLEU implementations may use different tokenization rules, text preprocessing steps, and other configurations, leading to different results. SacreBLEU offers a standardized approach to tokenization and score calculation, allowing researchers to compare results more objectively. It provides a signature that specifies the exact configuration used, making evaluations reproducible. SacreBLEU has become the preferred implementation for reporting BLEU scores in machine translation research and is widely adopted in the community to ensure consistent evaluation across different papers and systems. | [HuggingFace: SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu), [GitHub: SacreBLEU](https://github.com/mjpost/sacrebleu) |
| GLEU (Google BLEU) | Модификация метрики BLEU, разработанная Google. Отличается от стандартного BLEU тем, что учитывает не только n-граммы, совпадающие с эталоном, но и n-граммы, которые отсутствуют в эталоне. Это позволяет лучше оценивать качество грамматических исправлений, где важно не только то, что было добавлено, но и то, что было удалено. GLEU показывает более высокую корреляцию с человеческими оценками для задач грамматической коррекции. | GLEU (Google BLEU) is a modification of the BLEU metric developed by Google. It differs from standard BLEU in that it considers not only n-grams that match the reference but also n-grams that are absent from the reference. This allows for better evaluation of grammatical corrections, where it's important to consider both what was added and what was removed. GLEU calculates the overlap of n-grams between the predicted text and the reference, as well as between the source text and the reference. Originally designed for grammatical error correction tasks, GLEU shows better correlation with human judgments in scenarios where edits and corrections are being evaluated. The metric provides a more balanced assessment of fluency and adequacy in text correction tasks than standard BLEU. | [Google Research: GLEU](https://research.google/pubs/pub44565/), [GitHub: GLEU Implementation](https://github.com/cnap/gec-ranking) |
| chrF | Метрика для оценки качества машинного перевода, основанная на символьных n-граммах (вместо словных n-грамм, как в BLEU). Это делает её менее чувствительной к различным морфологическим изменениям в словах и особенно эффективной для языков с богатой морфологией. chrF считает F-меру на основе точности и полноты совпадения символьных n-грамм между переведенным и эталонным текстом. | chrF is a machine translation evaluation metric based on character n-grams (instead of word n-grams as in BLEU). This makes it less sensitive to various morphological changes in words and particularly effective for languages with rich morphology. chrF calculates an F-score based on the precision and recall of character n-gram matches between translated and reference texts. It has several variants, such as chrF++ which also incorporates word n-grams at a lower weight. Studies have shown that chrF correlates better with human judgments than BLEU for morphologically rich languages and can better capture adequacy in translation. The metric is language-independent and doesn't require tokenization, making it simpler to implement and more consistent across different language pairs. | [HuggingFace: chrF](https://huggingface.co/spaces/evaluate-metric/chrf), [GitHub: chrF Implementation](https://github.com/m-popovic/chrF) |
| TER (Translation Edit Rate) | Метрика для оценки качества машинного перевода, которая измеряет количество правок, необходимых для превращения машинного перевода в эталонный. Правки могут включать вставку, удаление, замену слов и перестановку слов или фраз. Чем меньше значение TER, тем лучше качество перевода. TER особенно полезен для оценки постредактирования и трудозатрат, необходимых для исправления машинного перевода. | TER (Translation Edit Rate) is a metric for evaluating machine translation quality by measuring the number of edits required to change a machine translation output into a reference translation. These edits can include insertion, deletion, substitution of words, and shifting of words or phrases. The TER score is calculated as the number of edits divided by the average number of reference words. Lower TER scores indicate better translation quality as fewer changes are needed to match the reference. TER is particularly useful for evaluating post-editing effort and the amount of human effort required to correct machine translations. Unlike metrics like BLEU that focus on n-gram matches, TER directly models the editing process that would be performed by a human reviewer, making it intuitive and interpretable. | [HuggingFace: TER](https://huggingface.co/spaces/evaluate-metric/ter), [Wikipedia: TER](https://en.wikipedia.org/wiki/Translation_edit_rate) |
| BLEURT | Метрика оценки качества генерации текста, разработанная Google Research, основанная на нейронных сетях и предварительном обучении. Отличается от традиционных метрик тем, что предварительно обучается на миллионах синтетических примеров, а затем дообучается на человеческих оценках. BLEURT способен улавливать семантические тонкости, которые не улавливаются другими метриками, и показывает высокую корреляцию с человеческими оценками. | BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) is a text generation evaluation metric developed by Google Research, based on neural networks and pre-training. Unlike traditional metrics, BLEURT is pre-trained on millions of synthetic examples and then fine-tuned on human judgments. This approach allows BLEURT to learn complex patterns that correspond to human quality assessments. It uses a transformer-based model (derived from BERT) to capture semantic similarities between generated text and references, even when there's little lexical overlap. BLEURT has demonstrated high correlation with human judgments across various text generation tasks, including machine translation and summarization. It's particularly effective at capturing nuanced aspects of text quality, such as fluency, coherence, and adequacy. | [HuggingFace: BLEURT](https://huggingface.co/spaces/evaluate-metric/bleurt), [Google Research: BLEURT](https://github.com/google-research/bleurt) |
| COMET | Метрика для оценки качества машинного перевода, основанная на нейросетевых моделях, которая рассматривает исходный текст, перевод и эталонный перевод вместе. Отличается от традиционных метрик тем, что учитывает контекст исходного предложения при оценке качества перевода. COMET обучается на человеческих оценках качества перевода и показывает высокую корреляцию с суждениями человека. Существуют различные версии COMET, включая модели, которые могут работать без эталонных переводов. | COMET (Crosslingual Optimized Metric for Evaluation of Translation) is a neural-based machine translation evaluation metric that considers the source text, translation, and reference translation together. Unlike traditional metrics, COMET takes into account the context of the original sentence when evaluating translation quality. The metric uses cross-lingual embeddings and is trained on human judgments of translation quality, showing high correlation with human assessments. The COMET framework includes various model variants, including reference-free models that can evaluate translations without reference translations. This makes COMET particularly valuable for low-resource languages or scenarios where high-quality references are unavailable. COMET has consistently outperformed traditional metrics in WMT (Workshop on Machine Translation) shared tasks, demonstrating its effectiveness in capturing translation quality aspects that align with human perception. | [HuggingFace: COMET](https://huggingface.co/spaces/evaluate-metric/comet), [GitHub: COMET](https://github.com/Unbabel/COMET) |
| F1 Score | F1-метрика (или F1-мера) – это метрика для оценки качества моделей классификации, которая представляет собой гармоническое среднее между точностью (precision) и полнотой (recall). Она рассчитывается как 2 * (precision * recall) / (precision + recall). F1-метрика особенно полезна для несбалансированных наборов данных, где простая точность (accuracy) может быть мислидинговой. | F1 Score is a metric for evaluating classification models that combines precision and recall into a single measure. It is calculated as the harmonic mean of precision and recall: 2 * (precision * recall) / (precision + recall). F1 score ranges from 0 to 1, where 1 represents perfect precision and recall. The metric is particularly useful for imbalanced datasets where accuracy alone might be misleading. In binary classification, precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positives identified among all actual positives. F1 score balances these two, ensuring that both false positives and false negatives are penalized. For multi-class classification problems, F1 score can be calculated by averaging (either macro, weighted, or micro averaging) the F1 scores of each class. | [HuggingFace: F1](https://huggingface.co/spaces/evaluate-metric/f1), [Wikipedia: F1 Score](https://en.wikipedia.org/wiki/F-score) |
| Accuracy | Точность (Accuracy) - это базовая метрика для оценки качества моделей классификации, которая измеряет долю правильных предсказаний от общего числа предсказаний. Рассчитывается как отношение количества правильно классифицированных экземпляров к общему количеству экземпляров. Точность дает хорошие результаты только на сбалансированных данных, где классы представлены примерно в равных пропорциях. На несбалансированных данных может давать искаженные результаты, так как модель может достичь высокой точности, просто предсказывая доминирующий класс. | Accuracy is a fundamental metric for classification models that measures the proportion of correct predictions among all predictions made. It is calculated as the number of correct predictions divided by the total number of predictions. Accuracy ranges from 0 to 1, where 1 indicates perfect classification. While intuitive and easy to understand, accuracy has limitations, particularly with imbalanced datasets. For example, in a dataset where 95% of instances belong to class A and 5% to class B, a model could achieve 95% accuracy by simply predicting class A for all instances, without actually learning to distinguish between classes. Due to this limitation, accuracy should be used alongside other metrics like precision, recall, and F1 score, especially when dealing with imbalanced data or when different types of errors have different costs. | [HuggingFace: Accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy), [Wikipedia: Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) |
| MAUVE | MAUVE (Measuring the GAP between neural text and human text) - это метрика для оценки качества генерации текста, которая измеряет разрыв между распределениями машинного и человеческого текста. Содержательно MAUVE использует квантизированное представление распределения текста в пространстве признаков нейросетевой модели и затем оценивает расхождение между этими распределениями. Преимущество MAUVE в том, что она учитывает не только качество отдельных генераций, но и статистическую близость распределений генерируемого и реального текста. | MAUVE (Measuring the GAP between neural text and human text) is a metric for evaluating the quality of text generation models by measuring the divergence between the distributions of machine-generated and human-written text. MAUVE quantizes text representations in a neural network feature space and then computes the divergence between these quantized distributions. The metric is designed to capture both fluency and semantic coherence, providing a holistic assessment of text generation quality. Unlike reference-based metrics that compare generated text against specific target texts, MAUVE evaluates how well the distribution of generated texts matches the distribution of human texts. This makes it particularly suited for open-ended generation tasks where multiple outputs could be considered valid. The MAUVE score ranges from 0 to 1, with higher values indicating better alignment between the distributions of generated and human text. | [HuggingFace: MAUVE](https://huggingface.co/spaces/evaluate-metric/mauve), [GitHub: MAUVE](https://github.com/krishnap25/mauve) |
| Precision | Точность (Precision) - это метрика для оценки качества моделей классификации, которая измеряет долю правильных положительных предсказаний среди всех положительных предсказаний модели. Рассчитывается как отношение истинно-положительных результатов (true positives) к сумме истинно-положительных и ложно-положительных результатов (true positives + false positives). Precision показывает, насколько модель избегает ложных срабатываний (ошибок первого рода) и особенно важна в задачах, где цена ложного срабатывания высока (например, в медицинской диагностике). | Precision is a metric for evaluating classification models that measures the proportion of true positive predictions among all positive predictions made by the model. It is calculated as the ratio of true positives to the sum of true positives and false positives (TP / (TP + FP)). Precision answers the question: "Of all instances predicted as positive, how many were actually positive?" It ranges from 0 to 1, where 1 indicates perfect precision (no false positives). High precision indicates a low false positive rate, which is important in applications where false positives are costly or harmful, such as spam detection (where marking legitimate emails as spam is problematic) or medical diagnosis (where false positive diagnoses can lead to unnecessary treatments). Precision is often used in conjunction with recall to provide a more comprehensive assessment of a model's performance, particularly for imbalanced datasets. While precision focuses on the accuracy of positive predictions, it does not account for false negatives (instances that should have been classified as positive but were missed by the model). | [HuggingFace: Precision](https://huggingface.co/spaces/evaluate-metric/precision), [Wikipedia: Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall) |
| Recall | Полнота (Recall) - это метрика для оценки качества моделей классификации, которая измеряет долю правильно определенных положительных примеров среди всех реальных положительных примеров в данных. Рассчитывается как отношение истинно-положительных результатов (true positives) к сумме истинно-положительных и ложно-отрицательных результатов (true positives + false negatives). Recall показывает, насколько модель способна обнаружить все положительные примеры и особенно важна в задачах, где цена пропуска положительного примера высока (например, при диагностике серьезных заболеваний). | Recall is a metric for evaluating classification models that measures the proportion of actual positive instances that were correctly identified by the model. It is calculated as the ratio of true positives to the sum of true positives and false negatives (TP / (TP + FN)). Recall answers the question: "Of all actual positive instances, how many did the model correctly identify?" It ranges from 0 to 1, where 1 indicates that all actual positive instances were correctly identified (no false negatives). High recall indicates a low false negative rate, which is important in applications where missing a positive instance is costly, such as disease diagnosis (where failing to detect a disease could be life-threatening) or fraud detection (where missing fraudulent activity has significant financial implications). While optimizing for high recall, a model might increase its false positive rate, as it becomes more aggressive in classifying instances as positive. Therefore, recall is typically used alongside precision to balance the trade-off between false positives and false negatives, especially through metrics like the F1 score that combine both precision and recall. | [HuggingFace: Recall](https://huggingface.co/spaces/evaluate-metric/recall), [Wikipedia: Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall) |
| ROC AUC | Площадь под кривой ROC (ROC AUC) - это метрика для оценки качества моделей классификации, которая измеряет способность модели различать классы независимо от выбранного порога классификации. ROC-кривая (кривая операционной характеристики приемника) строится путем отображения доли истинно-положительных результатов (true positive rate) относительно доли ложно-положительных результатов (false positive rate) при различных порогах классификации. Площадь под этой кривой (AUC) является мерой разделительной способности модели. Значение 0.5 указывает на случайную модель, а 1.0 - на идеальную модель. | ROC AUC (Receiver Operating Characteristic Area Under the Curve) is a performance metric for classification models that measures the model's ability to discriminate between classes regardless of the classification threshold chosen. The ROC curve is created by plotting the true positive rate (sensitivity) against the false positive rate (1-specificity) at various threshold settings. The area under this curve (AUC) provides a single scalar value representing the model's overall discrimination capacity. AUC values range from 0 to 1, where a value of 0.5 suggests no discrimination ability (equivalent to random guessing), and 1.0 indicates perfect discrimination. One key advantage of ROC AUC is its insensitivity to class imbalance, making it particularly useful for evaluating models trained on skewed datasets. It also assesses the model's performance across all possible classification thresholds, which is valuable when the optimal threshold is not known or may change. However, ROC AUC may be less informative when the costs of false positives and false negatives are significantly different, in which case precision-recall curves or F1 scores might be more appropriate. | [HuggingFace: ROC AUC](https://huggingface.co/spaces/evaluate-metric/roc_auc), [Wikipedia: ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) |
| MCC | Коэффициент корреляции Мэтьюса (MCC) - это метрика для оценки качества моделей бинарной классификации, которая учитывает все элементы матрицы ошибок: истинно-положительные, истинно-отрицательные, ложно-положительные и ложно-отрицательные результаты. MCC является коэффициентом корреляции между фактическими и предсказанными бинарными метками и возвращает значение от -1 до +1, где +1 означает идеальное предсказание, 0 - случайное, а -1 - полностью неверное. Особенно полезен на несбалансированных данных. | The Matthews Correlation Coefficient (MCC) is a quality metric for binary classification models that takes into account all four elements of the confusion matrix: true positives, false positives, true negatives, and false negatives. Unlike metrics such as accuracy or F1 score, MCC is generally regarded as a balanced measure that can be used even when the classes are of very different sizes. MCC returns a value between -1 and +1, where +1 represents a perfect prediction, 0 represents random prediction, and -1 indicates total disagreement between prediction and observation. The coefficient is calculated using the formula: MCC = (TP×TN - FP×FN) / √((TP+FP)(TP+FN)(TN+FP)(TN+FN)). This metric is particularly valuable in scenarios with highly imbalanced classes, where accuracy might be misleadingly high simply by predicting the majority class. Because MCC considers all four quadrants of the confusion matrix, it provides a more comprehensive assessment of model performance than metrics that focus only on a subset of these elements. MCC is widely used in bioinformatics and other fields where class imbalance is common. | [HuggingFace: MCC](https://huggingface.co/spaces/evaluate-metric/matthews_correlation), [Wikipedia: MCC](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) |
| MSE | Среднеквадратичная ошибка (MSE) - это метрика для оценки качества моделей регрессии, которая измеряет среднее значение квадратов разностей между прогнозируемыми и фактическими значениями. MSE рассчитывается путем вычисления квадрата разности между прогнозируемым и фактическим значением для каждого примера, суммирования этих квадратов и затем деления на количество примеров. Эта метрика особенно чувствительна к выбросам из-за квадратичной природы ошибки, что может быть как преимуществом (когда большие ошибки действительно критичны), так и недостатком (когда выбросы искажают оценку). MSE всегда неотрицательна, и значения, близкие к нулю, указывают на хорошее качество модели. | Mean Squared Error (MSE) is a metric for evaluating regression models that measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. MSE is calculated by taking the difference between the predicted and actual values for each example, squaring these differences, summing them up, and then dividing by the number of examples. A key characteristic of MSE is that it gives a higher weight to larger errors due to the squaring operation, making it particularly sensitive to outliers. This sensitivity can be either advantageous (when larger errors are indeed more critical to penalize) or disadvantageous (when outliers disproportionately influence the overall assessment). The MSE is always non-negative, and values closer to zero indicate better model performance. It is widely used in various fields including statistics, economics, and machine learning, particularly for optimization problems where the goal is to minimize prediction errors. One limitation of MSE is that it expresses error in units that are squared, which might be less interpretable than metrics like Mean Absolute Error (MAE) that maintain the original units. | [HuggingFace: MSE](https://huggingface.co/spaces/evaluate-metric/mean_squared_error), [Wikipedia: MSE](https://en.wikipedia.org/wiki/Mean_squared_error) |
| MAE | Средняя абсолютная ошибка (MAE) - это метрика для оценки качества моделей регрессии, которая измеряет среднее абсолютное отклонение между прогнозируемыми и фактическими значениями. MAE рассчитывается путем вычисления абсолютной разности между прогнозируемым и фактическим значением для каждого примера, суммирования этих абсолютных разностей и затем деления на количество примеров. В отличие от MSE, MAE менее чувствительна к выбросам, так как не использует квадрат ошибки, и выражается в тех же единицах, что и исходные данные, что облегчает интерпретацию. MAE всегда неотрицательна, и значения, близкие к нулю, указывают на хорошее качество модели. | Mean Absolute Error (MAE) is a metric for evaluating regression models that measures the average of the absolute differences between predictions and actual observations. MAE is calculated by taking the absolute difference between each predicted and actual value, summing these absolute differences, and then dividing by the number of observations. Unlike Mean Squared Error (MSE), MAE is less sensitive to outliers because it does not square the errors, which can disproportionately penalize large deviations. This makes MAE more robust in scenarios where occasional large errors should not dominate the overall assessment of model performance. Another advantage of MAE is that it maintains the same unit of measurement as the original data, making it more interpretable in practical applications. For example, if predicting temperature in degrees Celsius, the MAE will also be expressed in degrees Celsius, representing the average magnitude of error regardless of direction (over-prediction or under-prediction). Like MSE, MAE is always non-negative, with values closer to zero indicating better model performance. However, when optimizing models, MAE may lead to different solutions than MSE because it weighs all errors linearly rather than quadratically. | [HuggingFace: MAE](https://huggingface.co/spaces/evaluate-metric/mae), [Wikipedia: MAE](https://en.wikipedia.org/wiki/Mean_absolute_error) |
| R² | Коэффициент детерминации (R²) - это статистическая метрика, которая показывает, какую долю дисперсии зависимой переменной объясняет модель регрессии. R² вычисляется как 1 минус отношение суммы квадратов остатков к общей сумме квадратов отклонений. Значения R² варьируются от 0 до 1, где 1 означает идеальное предсказание (модель полностью объясняет всю изменчивость данных), 0 означает, что модель не объясняет изменчивость данных лучше, чем простое среднее значение. В некоторых случаях R² может быть отрицательным, что указывает на очень плохую модель. | R-squared (R²), also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. It provides an indication of how well the model fits the observed data. R² is calculated as 1 minus the ratio of the residual sum of squares to the total sum of squares. The values of R² range from 0 to 1, where a value of 1 indicates that the model explains all the variability in the response variable, and 0 indicates that the model explains none of the variability (i.e., the model is no better than simply predicting the mean of the dependent variable for all observations). In some cases, R² can be negative, indicating that the model performs worse than a horizontal line. One of the main advantages of R² is its intuitive interpretation as a percentage of explained variance, making it accessible even to those with limited statistical knowledge. However, R² has limitations: it will always increase when more predictors are added to a model, regardless of whether those predictors are actually meaningful, which can lead to overfitting. To address this issue, adjusted R² penalizes the addition of predictors that do not add value to the model. Despite its limitations, R² remains one of the most commonly used metrics for evaluating regression models due to its interpretability and widespread acceptance across various fields. | [HuggingFace: R²](https://huggingface.co/spaces/evaluate-metric/r_squared), [Wikipedia: R²](https://en.wikipedia.org/wiki/Coefficient_of_determination) |
| RMSE | Среднеквадратичное отклонение (RMSE) - это метрика для оценки качества моделей регрессии, которая представляет собой квадратный корень из среднеквадратичной ошибки (MSE). RMSE измеряет среднее квадратичное отклонение между предсказанными и фактическими значениями. Отличие от MSE заключается в том, что RMSE выражается в тех же единицах измерения, что и исходные данные, что делает её более интерпретируемой. Так же как и MSE, RMSE придает более высокий вес большим ошибкам из-за операции возведения в квадрат, что делает её чувствительной к выбросам. RMSE всегда неотрицательна, и значения, близкие к нулю, указывают на хорошее качество модели. | Root Mean Square Error (RMSE) is a standard metric for evaluating regression models that measures the square root of the average squared differences between predicted and actual values. RMSE is calculated by taking the square root of the Mean Squared Error (MSE), which involves computing the squared difference between each predicted and actual value, summing these squared differences, dividing by the number of observations, and then taking the square root of this quotient. A key advantage of RMSE over MSE is that it returns to the original units of the dependent variable, making it more interpretable in practical applications. For instance, if predicting house prices in dollars, the RMSE will also be expressed in dollars, representing the typical magnitude of prediction error. Like MSE, RMSE gives higher weight to larger errors due to the squaring operation, making it particularly sensitive to outliers. This property means that RMSE penalizes large errors more heavily than small ones, which can be desirable in contexts where large errors are especially problematic. RMSE is always non-negative, with values closer to zero indicating better model performance. It is widely used in various fields including meteorology, geosciences, and finance, particularly when the cost of errors increases quadratically with the size of the error. RMSE is also commonly used in model selection and comparison, with the model having the lower RMSE generally considered to be the better fit to the data. | [HuggingFace: RMSE](https://huggingface.co/spaces/evaluate-metric/rmse), [Wikipedia: Root-mean-square deviation](https://en.wikipedia.org/wiki/Root-mean-square_deviation) |
| MAPE | Средняя абсолютная процентная ошибка (MAPE) - это метрика для оценки качества моделей регрессии, которая измеряет среднюю абсолютную ошибку как процент от фактических значений. MAPE рассчитывается путем вычисления абсолютной процентной ошибки для каждого примера, суммирования этих значений и деления на количество примеров. Основное преимущество MAPE заключается в том, что она выражается в процентах, что позволяет легко интерпретировать и сравнивать результаты между разными наборами данных. Однако у MAPE есть ограничения: она не может быть использована, когда фактические значения близки к нулю или равны нулю, и она может придавать больший вес отрицательным ошибкам, чем положительным. | Mean Absolute Percentage Error (MAPE) is a metric for evaluating regression models that measures the size of error as a percentage of the actual value. MAPE is calculated by taking the absolute difference between the predicted and actual values, dividing by the actual value, multiplying by 100 to express as a percentage, and then averaging these percentage errors across all observations. The primary advantage of MAPE is its interpretability, as it expresses error in percentage terms, making it easy to communicate and understand across different scales and business contexts. A MAPE of 10% means that, on average, the model's predictions deviate from actual values by 10%. This unit-free nature allows for meaningful comparisons across different datasets and models. However, MAPE has several limitations. Most notably, it is undefined or problematic when actual values are zero or close to zero, as division by these values leads to infinity or very large errors. Additionally, MAPE treats positive and negative errors asymmetrically — a prediction that is half the actual value (50% too low) contributes a 50% error, while a prediction that is double the actual value (100% too high) contributes a 100% error, despite both being equally distant in absolute terms. MAPE also cannot distinguish between over-prediction and under-prediction. Despite these limitations, MAPE remains widely used in forecasting, particularly in business and finance, due to its intuitive percentage interpretation that is easily understood by non-technical stakeholders. | [HuggingFace: MAPE](https://huggingface.co/spaces/evaluate-metric/mape), [Wikipedia: MAPE](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) |
| Spearman | Коэффициент ранговой корреляции Спирмена (или коэффициент корреляции Спирмена, обозначается через ρ) - это непараметрическая мера статистической зависимости между ранжированиями двух переменных. Он оценивает, насколько хорошо может быть описана зависимость между двумя переменными с помощью монотонной функции, не делая никаких предположений о частотном распределении переменных. Коэффициент равен +1 или -1, когда каждая переменная является совершенной монотонной функцией другой переменной, и близок к нулю, когда между переменными нет монотонной связи. | Spearman's rank correlation coefficient, or Spearman's rho (ρ), is a nonparametric measure of statistical dependence between the rankings of two variables. It assesses how well the relationship between two variables can be described using a monotonic function, without making any assumptions about the frequency distributions of the variables. The coefficient ranges from +1 to -1, with +1 indicating a perfect positive monotonic correlation (as one variable increases, so does the other), -1 indicating a perfect negative monotonic correlation (as one variable increases, the other decreases), and 0 indicating no monotonic correlation. Unlike Pearson correlation, which measures linear relationships, Spearman correlation captures monotonic relationships, making it robust to outliers and applicable to data with non-normal distributions. To calculate Spearman's coefficient, the data for each variable is ranked separately, and then the Pearson correlation is calculated on these ranks. This process effectively transforms the data to mitigate the influence of extreme values or non-linear relationships. Spearman correlation is particularly useful in scenarios where the data does not follow a normal distribution, contains outliers, or when the relationship is monotonic but not necessarily linear. It is widely used in various fields including psychology, economics, and biological sciences to measure the strength and direction of association between ranked variables. One limitation of Spearman correlation is that it does not capture non-monotonic relationships; for instance, if a variable first increases and then decreases with respect to another variable, Spearman correlation might suggest no correlation even though a clear pattern exists. | [HuggingFace: Spearman](https://huggingface.co/spaces/evaluate-metric/spearmanr), [Wikipedia: Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) |
| Kendall Tau | Коэффициент ранговой корреляции Кендалла (тау-коэффициент Кендалла, обозначается через τ) - это непараметрическая мера ранговой корреляции, которая измеряет силу связи между двумя порядковыми переменными, основываясь на разнице между количеством согласующихся и несогласующихся пар в данных. Две пары данных считаются согласующимися, если расположение рангов для обеих элементов совпадает: если x₁ > x₂, то и y₁ > y₂, или если x₁ < x₂, то и y₁ < y₂. Они считаются несогласующимися, если x₁ > x₂, но y₁ < y₂, или если x₁ < x₂, но y₁ > y₂. | Kendall's rank correlation coefficient, commonly referred to as Kendall's tau (τ), is a nonparametric measure of the strength and direction of association that exists between two variables measured on at least an ordinal scale. The test is based on the number of concordant and discordant pairs in the data. A pair of observations is concordant if the ranks for both elements agree: if x₁ > x₂, then y₁ > y₂, or if x₁ < x₂, then y₁ < y₂. A pair is discordant if x₁ > x₂, but y₁ < y₂, or if x₁ < x₂, but y₁ > y₂. Kendall's tau is calculated as the difference between the number of concordant and discordant pairs, divided by the total number of pairs (normalizing factor that ensures the value falls between -1 and 1). Like Spearman's rank correlation, Kendall's tau ranges from -1 to +1, with values close to +1 indicating strong agreement between the two rankings, values close to -1 indicating strong disagreement, and values close to zero indicating the absence of association. While Spearman's rank correlation and Kendall's tau often yield similar results, Kendall's tau is generally preferred when the data contain a small number of observations with many tied ranks, as it handles ties more effectively. Kendall's tau is also considered to be more robust and efficient than Spearman's rank correlation in many situations. One of the key advantages of Kendall's tau is its interpretability: it represents the probability of concordance minus the probability of discordance for a randomly selected pair of observations. Additionally, Kendall's tau has well-defined statistical properties that make it suitable for testing hypotheses about independence. It is widely used in various fields, particularly when analyzing rankings or when the relationship between variables might not be linear. | [HuggingFace: Kendall](https://huggingface.co/spaces/evaluate-metric/kendall_tau), [Wikipedia: Kendall rank correlation coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) |
| Pearson | Коэффициент корреляции Пирсона (r) - это мера линейной зависимости между двумя переменными, которая показывает, насколько сильно они связаны друг с другом. Коэффициент Пирсона рассчитывается как ковариация двух переменных, деленная на произведение их стандартных отклонений. Значения коэффициента варьируются от -1 до +1, где +1 означает полную положительную линейную корреляцию, 0 означает отсутствие линейной корреляции, а -1 означает полную отрицательную линейную корреляцию. Важно отметить, что коэффициент Пирсона может обнаружить только линейные зависимости и чувствителен к выбросам. | Pearson correlation coefficient, also known as Pearson's r, is a measure of linear correlation between two sets of data. It is the covariance of two variables, divided by the product of their standard deviations. The coefficient's value ranges from -1 to +1, with +1 representing a perfect positive linear correlation, 0 indicating no linear correlation, and -1 representing a perfect negative linear correlation. Developed by Karl Pearson in the late 19th century, it is the most widely used correlation metric in statistics and has served as the foundation for numerous statistical techniques, including regression analysis and factor analysis. Pearson correlation assumes that both variables are normally distributed and have a linear relationship with each other. It is sensitive to outliers, which can significantly impact the correlation value, especially in small sample sizes. Additionally, it can only detect linear relationships; if two variables have a strong non-linear relationship, Pearson correlation might indicate weak or no correlation despite a clear pattern existing in the data. In machine learning and data science, Pearson correlation is often used for feature selection, to understand the relationships between variables, and as a basis for dimensionality reduction techniques like Principal Component Analysis (PCA). Its simplicity, interpretability, and established theoretical framework make it an essential tool in any data analyst's toolkit, though it should be used with awareness of its limitations, particularly when dealing with non-linear relationships or when data distributions deviate significantly from normality. When comparing prediction models, Pearson correlation can be used to assess how well the predicted values correlate with the actual values, providing insight into the model's ability to capture linear relationships in the data. | [HuggingFace: Pearson](https://huggingface.co/spaces/evaluate-metric/pearsonr), [Wikipedia: Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) |
| AUC | Площадь под кривой (AUC, Area Under the Curve) - это метрика, которая измеряет всю двумерную область под всей кривой ROC (Receiver Operating Characteristic) от (0,0) до (1,1). AUC предоставляет совокупную меру эффективности для всех возможных пороговых значений классификации. Значение AUC колеблется от 0 до 1, где 1 означает идеальную различимость классов, 0.5 означает различимость, не лучше случайной (эквивалентно случайному угадыванию), а значения меньше 0.5 указывают на то, что классификатор работает хуже случайного. AUC особенно полезна, когда важна ранжировка предсказаний, а не их абсолютные значения, или когда набор данных имеет несбалансированное распределение классов. | Area Under the Curve (AUC) is a performance metric for classification problems that measures the entire two-dimensional area underneath the entire ROC (Receiver Operating Characteristic) curve from (0,0) to (1,1). The ROC curve plots the True Positive Rate (TPR, also called sensitivity or recall) against the False Positive Rate (FPR, also called 1-specificity) at various threshold settings. AUC provides an aggregate measure of performance across all possible classification thresholds. The value of AUC ranges from 0 to 1, where a value of 1 indicates perfect discriminatory ability, 0.5 indicates no discriminative ability (equivalent to random guessing), and values below 0.5 suggest that the classifier performs worse than random guessing. One of the significant advantages of AUC is that it is threshold-invariant; it measures the quality of the model's predictions irrespective of what classification threshold is chosen. This makes it particularly useful in scenarios where ranking predictions is more important than absolute probabilities, or when working with imbalanced datasets where the class distribution is skewed. AUC is also insensitive to class distribution, meaning that if the proportion of positive to negative instances changes, the AUC will remain the same as long as the model correctly ranks the examples. However, AUC has some limitations: it can be overly optimistic for imbalanced datasets, it doesn't provide information about the actual predicted probabilities, and it doesn't directly translate to real-world metrics that might be more relevant in specific applications (like precision or F1 score at particular thresholds). Despite these limitations, AUC remains a widely used and valuable metric, especially for comparing different models and understanding their discriminatory power across a range of thresholds. | [HuggingFace: AUC](https://huggingface.co/spaces/evaluate-metric/roc_auc), [Wikipedia: Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) |
| Cohen's Kappa | Коэффициент каппа Коэна - это статистическая метрика, которая измеряет межэкспертную надежность для категориальных данных. Она учитывает возможность случайного согласования между оценщиками. В контексте машинного обучения она используется для измерения согласованности между предсказаниями модели и фактическими данными, учитывая случайное совпадение. Значения коэффициента каппа варьируются от -1 до +1, где +1 означает полное согласие, 0 означает согласие, не лучше чем при случайном совпадении, а отрицательные значения указывают на систематическое несогласие. | Cohen's Kappa is a statistic that measures inter-rater reliability for categorical items. It takes into account the possibility of agreement occurring by chance. In the context of machine learning, Cohen's Kappa is used to assess the agreement between a model's predictions and the actual labels, accounting for the agreement that might occur by chance. The coefficient ranges from -1 to +1, where +1 indicates perfect agreement, 0 indicates agreement equivalent to chance, and negative values indicate systematic disagreement. Cohen's Kappa is particularly useful for imbalanced classification problems where accuracy might be misleading. For example, in a dataset with 95% negative examples, a model that always predicts 'negative' would achieve 95% accuracy but would have a Kappa score of 0, indicating no agreement beyond chance. The formula for Cohen's Kappa is κ = (po - pe) / (1 - pe), where po is the observed agreement (equivalent to accuracy) and pe is the expected agreement based on chance. The interpretation of Kappa values varies by field, but generally: values < 0 indicate no agreement, 0-0.20 slight agreement, 0.21-0.40 fair agreement, 0.41-0.60 moderate agreement, 0.61-0.80 substantial agreement, and 0.81-1.0 almost perfect agreement. Cohen's Kappa is widely used in fields such as medical diagnosis, content analysis, and educational assessment where rater reliability is important. In machine learning, it serves as a more robust metric than accuracy for evaluating classifiers, especially when dealing with imbalanced datasets or when the cost of different types of errors varies. | [HuggingFace: Cohen Kappa](https://huggingface.co/spaces/evaluate-metric/cohen_kappa), [Wikipedia: Cohen's kappa coefficient](https://en.wikipedia.org/wiki/Cohen%27s_kappa) |
| PR AUC | Площадь под кривой точности-полноты (PR AUC) - это метрика, которая измеряет площадь под кривой Precision-Recall. Данная метрика особенно полезна для несбалансированных наборов данных, где классы распределены неравномерно. В отличие от ROC AUC, которая строит график на основе TPR и FPR, PR AUC фокусируется на точности и полноте, что делает её более чувствительной к изменениям в небольшом классе в несбалансированном наборе данных. PR AUC варьируется от 0 до 1, где более высокие значения указывают на лучшую производительность модели. В идеальной модели площадь под кривой PR будет равна 1, что означает совершенное прогнозирование с совершенной точностью и полнотой. | Precision-Recall Area Under Curve (PR AUC) is a performance metric for classification problems that measures the area under the Precision-Recall curve. The Precision-Recall curve plots precision (positive predictive value) against recall (sensitivity or true positive rate) at various threshold settings. PR AUC is particularly useful for imbalanced datasets where the number of negative examples greatly exceeds the number of positive examples. Unlike ROC AUC, which plots TPR against FPR, PR AUC focuses on precision and recall, making it more sensitive to improvements in the minority class in imbalanced datasets. In scenarios where identifying the positive class is more important than correctly classifying the negative class, PR AUC provides a more informative assessment of model performance than ROC AUC. PR AUC ranges from 0 to 1, with higher values indicating better model performance. In an ideal model, the area under the PR curve would be 1, representing perfect prediction with perfect precision and recall. The baseline for PR AUC is not 0.5 (as it is for ROC AUC) but rather the proportion of positive examples in the dataset. This means that a random classifier would achieve a PR AUC equal to the prevalence of the positive class. The PR AUC metric is commonly used in information retrieval, anomaly detection, and any domain where the cost of false positives and false negatives is significantly different, or where the positive class is rare but important to identify correctly. | [HuggingFace: PR AUC](https://huggingface.co/spaces/evaluate-metric/pr_auc), [Wikipedia: Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) |адь под кривой точности-полноты (Precision-Recall curve). PR AUC особенно полезна для оценки классификации при сильно несбалансированных наборах данных, где положительные примеры встречаются редко, а отрицательные примеры преобладают. В отличие от ROC AUC, которая может давать излишне оптимистичные результаты на несбалансированных данных, PR AUC чувствительна к соотношению положительных и отрицательных примеров. Значения PR AUC варьируются от 0 до 1, где 1 означает идеальное качество классификации. | Precision-Recall Area Under Curve (PR AUC) is a performance metric that summarizes the trade-off between the precision (positive predictive value) and recall (sensitivity) for different probability thresholds. It is calculated as the area under the precision-recall curve, which plots precision against recall at various threshold settings. PR AUC is particularly valuable for evaluating classification models on imbalanced datasets where the negative class significantly outnumbers the positive class. In such scenarios, traditional metrics like accuracy and even ROC AUC can be misleading, as they might not adequately reflect a model's ability to identify the minority class. While ROC AUC plots true positive rate against false positive rate (focusing on the classifier's ability to avoid false positives), PR AUC focuses on how well the model identifies positive instances without raising too many false alarms. The value of PR AUC ranges from 0 to 1, with higher values indicating better model performance. A perfect classifier would achieve a PR AUC of 1, representing perfect precision and recall at all thresholds. A baseline for PR AUC is the ratio of positive samples in the dataset (unlike ROC AUC, where the baseline is always 0.5), which makes PR AUC more informative for imbalanced problems. When comparing models using PR AUC, it's important to note that the metric is sensitive to class distribution changes. This sensitivity is actually a feature, not a bug, as it helps select models that perform well on the specific imbalance ratio encountered in the real-world data. PR AUC is widely used in information retrieval, medical diagnosis, and anomaly detection applications where the cost of missing a positive instance (false negative) is high, and the positive instances are rare. | [HuggingFace: PR AUC](https://huggingface.co/spaces/evaluate-metric/roc_auc), [Wikipedia: Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) |
| Exact Match | Точное соответствие (Exact Match) - это метрика, которая оценивает долю предсказаний, которые в точности совпадают с реальными значениями. Это двоичная метрика, которая присваивает 1 при полном совпадении и 0 в противном случае, даже если предсказание очень близко к правильному. Метрика широко используется в задачах обработки естественного языка, таких как ответы на вопросы, машинный перевод или генерация кода, где важна точность всего вывода. | Exact Match is a metric that evaluates the proportion of predictions that precisely match the ground truth. It is a binary measure where a score of 1 is given for complete identity and 0 for any deviation, regardless of how minor. In natural language processing tasks, Exact Match is commonly used for evaluating question answering systems, where it measures the percentage of questions for which the predicted answer exactly matches the correct answer. Unlike partial credit metrics (such as F1 score or BLEU), Exact Match is strict and unforgiving - a prediction must be character-for-character identical to the ground truth to receive credit. This strictness makes Exact Match particularly useful for tasks where precision is critical, such as in code generation or when factual accuracy is paramount. However, its binary nature can also be a limitation, as it fails to acknowledge near-misses or semantically correct answers with minor syntactic differences. For this reason, Exact Match is often used in conjunction with other more flexible metrics. In machine learning evaluations, Exact Match is straightforward to interpret: it directly represents the percentage of examples where the model got everything completely right. Despite its simplicity, it remains a valuable metric, especially for tasks where partial correctness has limited practical value. | [HuggingFace: Exact Match](https://huggingface.co/spaces/evaluate-metric/exact_match), [Papers with code: Exact Match](https://paperswithcode.com/task/question-answering) |
| ROUGE-L | ROUGE-L (Самая длинная общая подпоследовательность ROUGE) - это вариант метрики ROUGE, который использует концепцию наиболее длинной общей подпоследовательности (LCS) между сгенерированным и эталонным текстами. В отличие от ROUGE-N, который рассматривает перекрытие n-грамм, ROUGE-L не требует последовательных совпадений, а лишь совпадения в том же относительном порядке. Это позволяет ей лучше учитывать гибкость естественного языка и возможные перефразировки. ROUGE-L часто используется для оценки систем суммирования текста, машинного перевода и автоматического ответа на вопросы. | ROUGE-L (Longest Common Subsequence ROUGE) is a variant of the ROUGE metric family that uses the concept of Longest Common Subsequence (LCS) between the candidate and reference text. Unlike ROUGE-N, which counts the overlap of n-grams, ROUGE-L does not require consecutive matches, only matches in the same relative order. This flexibility allows ROUGE-L to capture sentence-level structure similarity and identify the longest co-occurring in-sequence n-grams automatically, without specifying a particular n-gram length. By focusing on the longest common subsequence, ROUGE-L can better account for word order and recognize when similar words appear in similar sequences, even if they're not adjacent. This makes it particularly useful for evaluating paraphrases, summaries with different wording, and translations that maintain the same general meaning but use different vocabulary or syntax. ROUGE-L produces three scores: precision (the LCS length divided by the length of the candidate text), recall (the LCS length divided by the length of the reference text), and an F-measure that combines both. The F-measure is typically reported as the ROUGE-L score, with higher values indicating better quality. One advantage of ROUGE-L over other ROUGE variants is that it requires no predefined n-gram length, making it more adaptable to different linguistic patterns and text structures. It can recognize longer patterns of similarity while still being sensitive to word order, which is important for preserving meaning. ROUGE-L is widely used in evaluating text summarization, machine translation, and question answering systems, providing a measure of fluency and content preservation that complements other metrics like BLEU or standard ROUGE-N scores. | [HuggingFace: ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge), [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf) |
| DER | Коэффициент ошибки диаризации (DER) - это метрика, используемая для оценки качества систем диаризации дикторов (определения "кто говорит когда" в аудиозаписи). DER рассчитывается как сумма трех типов ошибок: пропущенной речи (когда система не обнаруживает речь говорящего), ложной тревоги (когда система ошибочно определяет тишину как речь) и ошибки говорящего (когда система неправильно идентифицирует говорящего), поделенной на общее время речи. Чем ниже значение DER, тем лучше работает система диаризации. | Diarization Error Rate (DER) is a metric used to evaluate the performance of speaker diarization systems, which aim to answer the question "who spoke when" in an audio recording. DER is calculated as the sum of three types of errors: missed speech (when the system fails to detect speech that is present), false alarm (when the system incorrectly identifies non-speech as speech), and speaker error (when the system assigns speech to the wrong speaker), divided by the total duration of speech in the recording. The lower the DER, the better the diarization system. DER is expressed as a percentage, with 0% representing perfect diarization. In practical terms, DER quantifies the fraction of time that is incorrectly attributed to speakers, not attributed to any speaker when it should be, or attributed to a speaker when it shouldn't be. It's important to note that DER is typically calculated after an optimal mapping between the reference speaker labels and the system's speaker labels has been established, as the absolute speaker identities are usually arbitrary in diarization tasks. DER has been the standard evaluation metric for speaker diarization since its introduction in the early 2000s and is used in various speech processing challenges and benchmarks. However, it has limitations, particularly in how it handles overlapping speech (when multiple speakers talk simultaneously), which has led to the development of complementary metrics in recent research. | [HuggingFace: DER](https://huggingface.co/spaces/evaluate-metric/der), [NIST Rich Transcription Evaluations](https://www.nist.gov/itl/iad/mig/rich-transcription-evaluation) |
| CIDEr | CIDEr (Consensus-based Image Description Evaluation) - это метрика, разработанная специально для оценки качества генерации текстовых описаний изображений. В отличие от многих других метрик, CIDEr учитывает консенсус между несколькими эталонными описаниями, созданными людьми. Метрика использует TF-IDF (Term Frequency-Inverse Document Frequency) взвешивание, чтобы придать большее значение информативным словам и меньшее - общим словам, часто встречающимся во всех описаниях. CIDEr измеряет сходство между n-граммами сгенерированного описания и n-граммами эталонных описаний, с учетом их TF-IDF весов. Значения CIDEr обычно варьируются от 0 до 10, где более высокие значения указывают на лучшее соответствие между сгенерированными и эталонными описаниями. | CIDEr (Consensus-based Image Description Evaluation) is a metric specifically designed for evaluating image captioning systems. Unlike many other evaluation metrics that were adapted from machine translation or text summarization, CIDEr was created with the unique challenges of image description in mind. The key insight behind CIDEr is that a good image description should capture the same information that humans would consider important when describing an image. To achieve this, CIDEr uses a consensus-based approach that measures how well a generated caption matches multiple human-written reference captions for the same image. CIDEr employs TF-IDF (Term Frequency-Inverse Document Frequency) weighting to emphasize informative words that occur in the references for a specific image but are uncommon in the wider corpus. This helps to reward captions that capture distinctive aspects of an image rather than generic descriptions. The metric computes similarity between n-grams (typically 1-4 grams) in the candidate caption and reference captions, weighted by their TF-IDF scores. These similarity scores are then averaged across different n-gram lengths to produce the final CIDEr score. CIDEr scores typically range from 0 to 10, with higher values indicating better performance. A score of 10 would represent perfect agreement with human references, though in practice, state-of-the-art systems achieve scores significantly lower than this ceiling. CIDEr has become one of the standard evaluation metrics in image captioning research, often used alongside other metrics like BLEU, METEOR, and ROUGE-L. However, it is generally considered more appropriate for image captioning tasks because of its design considerations specific to this domain. | [HuggingFace: CIDEr](https://huggingface.co/spaces/evaluate-metric/cider), [CIDEr: Consensus-based Image Description Evaluation](https://arxiv.org/abs/1411.5726) |
| SPICE | SPICE (Semantic Propositional Image Caption Evaluation) - это метрика для оценки качества генерации описаний изображений, которая фокусируется на семантическом содержимом, а не на лексическом сходстве. Метрика работает путем преобразования как сгенерированного, так и эталонных описаний в графы семантических сцен (scene graphs), которые представляют объекты, атрибуты и отношения в тексте. Затем SPICE сравнивает эти графы, используя F-меру, чтобы определить, насколько хорошо совпадают семантические пропозиции. Этот подход позволяет SPICE лучше оценивать семантическую точность описаний, чем метрики, основанные на n-граммах, такие как BLEU или CIDEr. | SPICE (Semantic Propositional Image Caption Evaluation) is a metric designed for evaluating image captioning systems that focuses on semantic similarity rather than lexical or syntactic overlap. While metrics like BLEU, METEOR, and CIDEr primarily assess text similarity through n-gram matching, SPICE measures how well the generated captions capture the same semantic content as human-written references. The core innovation of SPICE is its use of scene graphs as an intermediate representation. Both candidate and reference captions are parsed into scene graphs, which are abstract representations of the objects, attributes, and relationships described in the text. For example, "A brown dog is running on the beach" would be converted into a graph with nodes for "dog" and "beach," with attributes like "brown" attached to "dog," and a relationship "running on" connecting "dog" to "beach." SPICE then computes an F-score based on the matching tuples between the candidate and reference scene graphs. This approach allows SPICE to recognize when two differently worded captions convey the same semantic meaning, even if they use different vocabulary or syntax. SPICE scores range from 0 to 1, with higher values indicating better semantic similarity. Research has shown that SPICE correlates better with human judgments of caption quality than purely lexical metrics, especially when evaluating whether captions accurately represent the important objects and relationships in an image. Despite its advantages, SPICE has limitations. The parsing process can introduce errors, and the metric may not fully capture fluency or grammatical correctness. Additionally, the computational overhead of parsing sentences into scene graphs makes SPICE more resource-intensive than simpler metrics. In practice, SPICE is often used in conjunction with other metrics like CIDEr to provide a more comprehensive evaluation of image caption quality, with SPICE focusing on semantic accuracy and CIDEr capturing fluency and consensus with human descriptions. | [HuggingFace: SPICE](https://huggingface.co/spaces/evaluate-metric/spice), [SPICE: Semantic Propositional Image Caption Evaluation](https://arxiv.org/abs/1607.08822) |
| SARI | SARI (System output Against References and against the Input utterance) - это метрика, специально разработанная для оценки систем упрощения текста (text simplification). В отличие от других метрик, SARI учитывает не только сходство с эталонными упрощенными версиями, но и различия между исходным и упрощенным текстом. SARI измеряет качество упрощения, сравнивая n-граммы, которые были добавлены, удалены или сохранены в системном выводе по сравнению с исходным текстом и эталонными упрощениями. Такой подход позволяет метрике оценивать как грамматическую правильность, так и степень упрощения текста. | SARI (System output Against References and against the Input utterance) is a metric specifically designed for evaluating text simplification systems. Unlike traditional metrics that only compare system output against references, SARI explicitly measures the quality of simplification operations by comparing the system output against both the input text and reference simplifications. SARI evaluates three key aspects of text simplification: what words are added, what words are deleted, and what words are kept. It does this by computing the F1 scores for each of these operations at the n-gram level (typically using unigrams, bigrams, and trigrams). By comparing these operations against both the input and references, SARI can distinguish between good simplifications (which make appropriate additions, deletions, and retentions) and poor simplifications (which may be too similar to the input or make inappropriate changes). The final SARI score is the arithmetic mean of the F1 scores for these three operations. SARI scores range from 0 to 100, with higher scores indicating better simplification quality. SARI was developed to address limitations in using metrics like BLEU for simplification tasks. While BLEU primarily rewards similarity to references, SARI explicitly rewards dissimilarity from the complex input where appropriate. This is crucial for text simplification, where a system should make meaningful changes to the input while preserving its core meaning. Research has shown that SARI correlates better with human judgments of simplification quality than metrics like BLEU or FKGL (Flesch-Kincaid Grade Level). It's particularly effective at capturing the grammaticality, meaning preservation, and simplicity that characterize good text simplifications. SARI has become the standard evaluation metric for text simplification systems and is widely used in benchmarking datasets like WikiLarge and Newsela. | [HuggingFace: SARI](https://huggingface.co/spaces/evaluate-metric/sari), [Optimizing Statistical Machine Translation for Text Simplification](https://aclanthology.org/Q16-1029.pdf) |
| WiLI | WiLI (Идентификация языка на уровне слов) - это метрика, используемая для оценки систем идентификации языка в тексте. Она измеряет способность модели определить, на каком языке написан текст или фрагмент текста. В контексте набора данных WiLI, метрика оценивает точность распознавания среди 235 различных языков. Ключевым показателем является точность классификации, которая показывает, насколько часто модель правильно определяет язык данного текста. | WiLI (Word-level Language Identification) is a metric used to evaluate language identification systems. It measures a model's ability to determine the language of a given text or text fragment. In the context of the WiLI dataset, the metric assesses the accuracy of language recognition across 235 different languages. Language identification is a fundamental task in natural language processing that serves as a preprocessing step for many multilingual applications. The WiLI benchmark evaluates how well systems can distinguish between languages, even when the input text is very short (at the word or sentence level) or when the languages are closely related. The primary measure used in WiLI is classification accuracy, which indicates how often the model correctly identifies the language of a given text. Other important evaluation aspects include the confusion matrix (showing which languages are commonly mistaken for each other) and performance on low-resource languages (those with fewer available training examples). Modern language identification systems evaluated with WiLI typically use character-level features, language models, or deep learning approaches to capture the distinctive patterns of different languages. High-performing systems can achieve over 95% accuracy across the full set of 235 languages, though performance varies significantly based on factors such as text length, language similarity, and the presence of code-switching or mixed-language content. The WiLI benchmark is particularly valuable for evaluating multilingual NLP systems that need to process content in diverse languages, including low-resource and less commonly taught languages that might be underrepresented in other benchmarks. | [HuggingFace: WiLI](https://huggingface.co/spaces/evaluate-metric/wili), [WiLI-2018: Wiktionary Language Identification](https://arxiv.org/abs/1801.07779) |
| PERPLEXITY | Перплексия (Perplexity) - это метрика, используемая для оценки качества вероятностных моделей языка, таких как модели n-грамм или нейронные языковые модели. Перплексия измеряет насколько хорошо модель предсказывает выборку (например, текст). Она определяется как экспонента от средней отрицательной логарифмической вероятности на слово. Чем ниже значение перплексии, тем лучше модель предсказывает данные. Перплексия часто используется для сравнения различных языковых моделей и оценки естественности генерируемого текста. | Perplexity is a measurement used to evaluate the quality of probabilistic language models such as n-gram models or neural language models. At its core, perplexity quantifies how well a probability model predicts a sample (typically text). Mathematically defined as the exponentiated average negative log-likelihood per token, perplexity can be intuitively understood as measuring the model's "confusion" or "surprise" when predicting the next word in a sequence. Lower perplexity values indicate better model performance. For example, a perplexity of 100 means that the model is as confused as if it had to choose uniformly and randomly from 100 possibilities for each token. A perplexity of 1 (the theoretical minimum) would indicate that the model perfectly predicts the test sequence with absolute certainty. Perplexity has been a standard evaluation metric in language modeling since the early days of statistical NLP. It's particularly useful because it's calculated directly from the model's probability assignments without requiring additional annotations or resources. Despite its widespread use, perplexity has limitations. It only measures how well the model assigns probabilities to existing text and doesn't directly evaluate the quality or usefulness of text generated by the model. Additionally, perplexity values aren't directly comparable across different vocabularies or tokenization schemes. In modern NLP research, perplexity is often complemented with task-specific metrics and human evaluations, especially for generative models. Nevertheless, it remains a fundamental metric for evaluating and comparing language models, providing a quantitative measure of a model's predictive capabilities. | [HuggingFace: Perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity), [Language Models: Perplexity](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94) |
| CHRF | CHRF (Character n-gram F-score) - это метрика оценки качества машинного перевода, которая работает на уровне символов, а не слов. В отличие от слово-ориентированных метрик, таких как BLEU или METEOR, CHRF сравнивает n-граммы символов в сгенерированном и эталонном переводах. Это делает её особенно эффективной для морфологически богатых языков, где небольшие изменения в словах (например, окончания) могут значительно изменить значение. CHRF рассчитывается как F-мера между символьными n-граммами сгенерированного и эталонного переводов. | CHRF (Character n-gram F-score) is a metric for evaluating machine translation quality that operates at the character level rather than the word level. While most traditional translation metrics like BLEU work with word n-grams, CHRF compares character n-grams between candidate and reference translations. This approach offers several advantages, particularly for morphologically rich languages where small variations in word forms (such as inflections or affixes) are common and semantically significant. The metric works by computing the F-score of character n-grams that overlap between the machine translation output and reference translations. Typically, CHRF considers n-grams ranging from 1 to 6 characters in length. The formula balances precision (proportion of character n-grams in the candidate translation that appear in the reference) and recall (proportion of character n-grams in the reference that appear in the candidate), with beta parameter typically set to 2 to give more weight to recall. CHRF scores range from 0 to 100, with higher scores indicating better translations. Studies have shown that CHRF correlates better with human judgments than BLEU for many language pairs, especially for morphologically complex languages like Finnish, Turkish, or Russian, where small character-level changes can significantly alter meaning. The character-level approach also makes CHRF more robust to word compounding (common in languages like German) and tokenization issues. CHRF has become increasingly popular in machine translation evaluation, with variants like CHRF++ (which incorporates both character and word n-grams) offering further refinements. Its strong correlation with human judgments, language independence, and simplicity of calculation have made it a standard metric in translation evaluation alongside BLEU and METEOR. In practical applications, CHRF often provides a more fine-grained and linguistically informed assessment of translation quality, capturing subtle differences that word-based metrics might miss. | [HuggingFace: CHRF](https://huggingface.co/spaces/evaluate-metric/chrf), [Character-Based BLEU for Automatic MT Evaluation](https://aclanthology.org/W15-3049.pdf) |
| GLEU | GLEU (Google BLEU) - это модификация метрики BLEU, разработанная Google для оценки качества грамматического исправления текста и машинного перевода. В отличие от стандартного BLEU, GLEU учитывает не только n-граммы, которые были правильно созданы в выходном тексте, но и n-граммы, которые были правильно удалены из исходного текста. Это делает GLEU особенно полезным для задач, где целью является модификация исходного текста (например, исправление грамматических ошибок), а не создание полностью нового текста с нуля. GLEU рассчитывает пересечение n-грамм между выходным текстом и эталонным текстом, исключая n-граммы, которые также встречаются в исходном тексте. | GLEU (Google BLEU) is a variant of the BLEU metric designed by Google specifically for evaluating grammatical error correction systems and machine translation. Unlike standard BLEU, which only measures n-gram precision between the system output and reference translations, GLEU considers both precision and recall of n-grams with respect to the source and reference texts. GLEU calculates the n-gram overlap between the system output and reference, excluding n-grams that are also present in the source text. This approach directly rewards systems for making correct changes (adding good n-grams and removing bad n-grams) while penalizing unnecessary changes. The metric produces scores ranging from 0 to 1, with higher scores indicating better performance. Mathematically, GLEU is calculated by comparing the n-grams in the system output that match the reference but not the source (rewards for good corrections) and the n-grams that match both the reference and source (rewards for maintaining correct parts). GLEU was specifically designed to address limitations in using BLEU for grammatical error correction tasks. While BLEU tends to favor minimal edits (since it primarily rewards precision), GLEU provides a more balanced evaluation by considering both precision and recall components. Research has shown that GLEU correlates better with human judgments for grammatical error correction tasks than traditional metrics like BLEU, F-score, or I-measure. The metric has been widely adopted for evaluating grammatical error correction systems in shared tasks and benchmarks. It's particularly effective for evaluating systems where subtle modifications to text are required, as opposed to generating entirely new text. GLEU's ability to capture the nuance of text correction makes it valuable for applications beyond grammatical error correction, including text simplification, style transfer, and certain aspects of machine translation evaluation. | [HuggingFace: GLEU](https://huggingface.co/spaces/evaluate-metric/gleu), [GLEU: Automatic Evaluation of Sentence-Level Fluency](https://aclanthology.org/P15-2073.pdf) |
| BERTScore | BERTScore - это метрика оценки качества текста, использующая предобученные модели BERT для вычисления сходства между генерируемым и эталонным текстами на основе контекстных эмбеддингов. В отличие от традиционных метрик, основанных на точном совпадении слов или n-грамм, BERTScore учитывает семантическое сходство и контекстную информацию. Это позволяет более точно оценивать тексты, использующие синонимы или перефразирование. Метрика работает путем вычисления попарного косинусного сходства между токенами из двух текстов и нахождения оптимального соответствия между ними. | BERTScore is a text evaluation metric that leverages pre-trained BERT models to compute similarity between candidate and reference texts based on their contextual embeddings. Unlike traditional n-gram overlap metrics like BLEU or ROUGE, BERTScore captures semantic similarity and contextual information, making it more robust to paraphrasing and synonym usage. The metric works by computing cosine similarity between token embeddings from the candidate and reference texts using a pre-trained language model like BERT, RoBERTa, or other Transformer-based models. For each token in the candidate text, BERTScore finds the most similar token in the reference text (and vice versa), then aggregates these similarity scores to compute precision, recall, and F1 measures. By using contextual embeddings, BERTScore can recognize when different words express the same meaning in context - a capability that traditional string-matching metrics lack. For example, it can recognize that "the cat is on the mat" and "the feline is sitting on the rug" convey similar meaning despite having few words in common. Research has shown that BERTScore correlates better with human judgments across various text generation tasks compared to n-gram based metrics. The metric is particularly effective for evaluating tasks where semantic equivalence matters more than exact wording, such as machine translation, summarization, and image captioning. BERTScore offers several advantages: it requires no training, works for any language with available pre-trained models, and provides more interpretable results through visualization of token-level matches between texts. However, it's computationally more expensive than traditional metrics and performance can vary depending on the specific pre-trained model used. Since its introduction in 2019, BERTScore has become a standard evaluation tool in many natural language generation benchmarks, complementing traditional metrics for a more comprehensive assessment of text quality. | [HuggingFace: BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore), [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675) |
| BLEURT | BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) - это метрика оценки качества машинного перевода и генерации текста, разработанная Google Research. В отличие от BERTScore, который использует предобученные эмбеддинги напрямую, BLEURT представляет собой модель, которая дополнительно обучается на человеческих оценках и синтетических данных. BLEURT начинается с предобученной модели BERT, которая затем проходит многозадачное предобучение на синтетических данных, а затем тонкую настройку на реальных человеческих оценках. Это позволяет ей учитывать не только семантическое сходство, но и другие аспекты качества текста, такие как грамматическая правильность и стиль. | BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) is a text evaluation metric developed by Google Research for assessing the quality of machine translation and text generation. Unlike earlier metrics such as BLEU that rely on exact n-gram matching, or BERTScore that directly uses pretrained embeddings, BLEURT represents a learned approach that combines the power of contextual representations with supervised training on human judgments. BLEURT starts with a pretrained BERT model, which is then fine-tuned in a multi-task learning setup first on millions of synthetic sentence pairs (created through controlled perturbations like word dropping, substitution, and backtranslation), followed by fine-tuning on human ratings of text quality. This two-stage training approach enables BLEURT to capture subtle aspects of text quality beyond semantic similarity, including fluency, grammar, and style. The metric takes a reference sentence and a candidate sentence as input, and outputs a quality score that correlates with human judgments. BLEURT's key innovation lies in its robustness to quality drift and domain shifts - challenges that often occur in machine translation and text generation systems. By incorporating a diverse range of synthetic training examples that simulate various types of errors, BLEURT can generalize well to new domains and types of errors not seen during training. Research has shown that BLEURT achieves higher correlation with human judgments compared to earlier metrics across different text generation tasks, particularly for machine translation and summarization. It's especially effective at distinguishing between translations or generations of different quality levels when the differences are subtle but still important to human readers. As an end-to-end learned metric, BLEURT continues to evolve with newer versions incorporating more sophisticated Transformer architectures and larger pretraining datasets. While it requires more computational resources than traditional metrics, its superior correlation with human judgments makes it valuable for research and development of text generation systems where accurate quality assessment is crucial. | [HuggingFace: BLEURT](https://huggingface.co/spaces/evaluate-metric/bleurt), [BLEURT: Learning Robust Metrics for Text Generation](https://arxiv.org/abs/2004.04696) |
| COMET | COMET (Crosslingual Optimized Metric for Evaluation of Translation) - это метрика оценки качества машинного перевода, основанная на нейронных сетях. В отличие от традиционных метрик, таких как BLEU, которые рассматривают только перевод и эталон, COMET учитывает также исходный текст. Метрика обучается на человеческих оценках качества перевода, что позволяет ей лучше коррелировать с человеческим восприятием качества. COMET использует многоязычные предобученные модели (например, XLM-RoBERTa) для получения эмбеддингов предложений, затем объединяет их и пропускает через полносвязные слои для предсказания оценки качества. | COMET (Crosslingual Optimized Metric for Evaluation of Translation) is a neural-based metric for evaluating machine translation quality. It represents a significant advancement over traditional metrics like BLEU or METEOR by incorporating deep learning techniques and modeling translation quality evaluation as a regression task. One of COMET's key innovations is that it considers the source text, machine translation output, and reference translation simultaneously, rather than just comparing the output against a reference as most traditional metrics do. This approach allows it to better capture translation adequacy (how well meaning is preserved from source to translation) alongside fluency. The metric works by leveraging large pretrained multilingual models like XLM-RoBERTa to encode the source, translation, and reference sentences into vector representations. These representations are then combined and processed through feed-forward layers to predict human-like quality scores. By training on datasets of human judgments, COMET learns to emulate how human evaluators would rate translation quality. Research has consistently shown that COMET achieves higher correlation with human judgments than traditional metrics across various language pairs and domains. It's particularly effective at capturing nuanced translation errors that n-gram based metrics might miss, such as semantic errors that preserve surface form but alter meaning. COMET is also more robust to legitimate translation variation - it can recognize when a translation differs from the reference but is still valid and high-quality. The metric has several variants, including reference-based models (which use source, candidate, and reference) and reference-free models (which only use source and candidate). This flexibility makes it applicable in scenarios where reference translations aren't available. Due to its strong performance, COMET has been rapidly adopted in major machine translation benchmarks and evaluation campaigns, including WMT (Workshop on Machine Translation). Its ability to provide more reliable automatic evaluation helps researchers and developers improve translation systems more efficiently by providing feedback that more closely aligns with human perception of translation quality. | [HuggingFace: COMET](https://huggingface.co/spaces/evaluate-metric/comet), [COMET: A Neural Framework for MT Evaluation](https://arxiv.org/abs/2009.09025) |
| METEOR | METEOR (Metric for Evaluation of Translation with Explicit Ordering) - это метрика оценки качества машинного перевода, которая решает некоторые недостатки BLEU, такие как оценка только точного совпадения слов. METEOR использует не только точные совпадения слов, но и их основы (стемминг), синонимы и парафразы. Метрика также придает больший вес порядку слов, что позволяет лучше учитывать беглость речи. METEOR работает путем сопоставления слов между машинным переводом и эталоном, создания выравнивания, а затем вычисления оценки на основе точности, полноты и штрафа за нарушение порядка. Это позволяет ей лучше коррелировать с человеческими оценками качества перевода, чем метрики, учитывающие только n-граммы. | METEOR (Metric for Evaluation of Translation with Explicit Ordering) is a machine translation evaluation metric designed to address several shortcomings of BLEU. While BLEU focuses primarily on precision of n-gram matches between a candidate translation and reference translations, METEOR incorporates additional linguistic features and gives equal weight to both precision and recall. The metric works by creating an alignment between the words in the candidate and reference translations. This alignment is based not only on exact word matches but also on stemming (matching word variants with the same root), synonym matching (using WordNet), and paraphrase tables. This linguistic flexibility allows METEOR to recognize when translations express the same meaning using different words or structures. METEOR's calculation involves two main components: a harmonized F-score based on precision and recall (with recall typically weighted higher than precision), and a penalty factor that accounts for word order differences. The penalty is calculated by grouping matches into chunks and penalizing translations with matches that are scattered rather than sequential, capturing aspects of grammatical correctness and fluency. Research has shown that METEOR achieves higher correlation with human judgments than metrics like BLEU, particularly at the sentence and segment level. This makes it especially valuable for evaluating individual translations rather than just system-level performance. METEOR has several versions that have evolved over time, with later versions incorporating more sophisticated linguistic resources and parameter tuning for different target languages. The metric is language-dependent in that it requires language-specific resources like stemmers and synonym databases, which has led to optimized versions for various languages beyond English. While more computationally expensive than BLEU due to its linguistic processing components, METEOR offers a more nuanced evaluation that better captures semantic equivalence between translations. It has become a standard part of machine translation evaluation alongside other metrics, providing complementary information about translation quality. | [HuggingFace: METEOR](https://huggingface.co/spaces/evaluate-metric/meteor), [The METEOR Metric for Automatic Evaluation of Machine Translation](https://aclanthology.org/W05-0909.pdf) |
| TER | TER (Translation Edit Rate) - это метрика оценки качества машинного перевода, которая измеряет количество правок, необходимых для превращения машинного перевода в эталонный перевод. Операции редактирования включают вставку, удаление, замену и перемещение слов. TER рассчитывается как минимальное количество операций редактирования, деленное на среднее количество слов в эталонном переводе. Одним из ключевых преимуществ TER является его интуитивность: значение TER можно интерпретировать как процент слов, которые необходимо исправить. | TER (Translation Edit Rate) is a metric for evaluating machine translation quality that measures the number of edits required to transform a machine translation output into a reference translation. Unlike metrics like BLEU or METEOR that focus on matching n-grams or linguistic features, TER directly quantifies the amount of human post-editing effort theoretically needed to fix a translation. The metric works by calculating the minimum number of edits required to match the reference, where edits include four operations: insertions, deletions, substitutions, and shifts (moving sequences of words). TER is computed as the number of edits divided by the average number of words in the reference, resulting in a score that can be interpreted as the percentage of words that need to be corrected. A lower TER score indicates better translation quality, with a score of 0 meaning the translation matches the reference perfectly. One of TER's key advantages is its interpretability for human translators and post-editors, as it directly relates to the practical effort required to correct translations. The metric also captures word order errors more effectively than some other metrics by including the shift operation, which counts moving a contiguous sequence of words to another location as a single edit rather than multiple insertions and deletions. TER has several variants, including TER-Plus (TERp) which extends the basic TER algorithm by incorporating paraphrases, stemming, and synonyms, making it more flexible in recognizing acceptable translation variants. Human-targeted TER (HTER) is another variant where human annotators create a reference that is maximally similar to the machine translation while being fluent and semantically equivalent to the source, providing a more accurate estimation of required post-editing. TER is widely used in the machine translation industry, particularly in contexts where post-editing efficiency is important. It's also commonly reported in translation quality evaluation campaigns alongside other metrics like BLEU and METEOR. While TER correlates reasonably well with human judgments of quality, it doesn't directly measure semantic accuracy or fluency. This limitation has led to its use as part of a suite of complementary metrics rather than in isolation. | [HuggingFace: TER](https://huggingface.co/spaces/evaluate-metric/ter), [Translation Edit Rate: A New Measurement of Translation Difficulty](https://www.cs.umd.edu/~snover/pub/amta06/ter_amta.pdf) |
| SacreBLEU | SacreBLEU - это стандартизированная реализация метрики BLEU, разработанная для устранения несогласованностей в имплементациях BLEU и обеспечения воспроизводимости результатов. Основные преимущества SacreBLEU включают: 1) стандартизированную предобработку текста, включая токенизацию и нормализацию; 2) отслеживаемый хэш предобработанных эталонных переводов, что позволяет убедиться, что исследователи оценивают одинаковые данные; 3) прозрачные параметры, которые документируются вместе с оценкой. SacreBLEU позволяет исследователям точно воспроизводить результаты друг друга, что способствует сопоставимости различных исследований в области машинного перевода. | SacreBLEU is a standardized implementation of the BLEU metric designed to address inconsistencies in BLEU implementations and ensure reproducibility across research papers. While the original BLEU metric has been widely adopted for evaluating machine translation quality, different preprocessing steps and implementation details often lead to varying scores when evaluating the same systems, making direct comparisons between research papers difficult. SacreBLEU solves this problem through several key features: First, it provides a standardized text preprocessing pipeline, including consistent tokenization and normalization rules that are language-aware. Second, it computes and reports a signature hash of the reference translations after preprocessing, allowing researchers to verify they're evaluating on identical data. Third, it documents all parameters (such as smoothing, case sensitivity, and tokenization method) alongside the reported score. The metric produces scores from 0 to 100, with higher scores indicating better translation quality. Like standard BLEU, SacreBLEU calculates the geometric mean of n-gram precisions (typically for n=1 to 4), with a brevity penalty applied to penalize overly short translations. SacreBLEU has become the de facto standard for reporting BLEU scores in machine translation research, particularly since major conferences like WMT (Conference on Machine Translation) have standardized on it. It supports a wide range of languages and tokenization schemes, including character-based tokenization for languages without clear word boundaries like Chinese and Japanese. Beyond standard BLEU, SacreBLEU also implements variants like SentenceBLEU for sentence-level evaluation and BLEU-specific tokenizers optimized for different languages. It has helped make machine translation evaluation more transparent and reproducible by eliminating the "BLEU score lottery" where implementation details could significantly affect reported results. This standardization has been instrumental in allowing the research community to make more reliable comparisons between different machine translation approaches and track progress in the field more accurately. | [HuggingFace: SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu), [SacreBLEU: A Protected BLEU Implementation for Machine Translation](https://aclanthology.org/W18-6319/) |
| GLEU | GLEU (Google-BLEU) - это метрика оценки качества машинного перевода, разработанная Google как альтернатива BLEU. GLEU является модификацией BLEU, которая лучше коррелирует с человеческими оценками на уровне предложений. GLEU рассчитывает n-граммное совпадение между машинным переводом и набором эталонных переводов, но, в отличие от BLEU, также учитывает n-граммы, которые присутствуют в машинном переводе, но отсутствуют в эталонных переводах. Это делает метрику более чувствительной к ошибкам и позволяет лучше различать качество переводов на уровне отдельных предложений. | GLEU (Google-BLEU) is a metric for evaluating machine translation quality developed by Google as an alternative to BLEU. It was designed to address BLEU's limitations at the sentence level, where standard BLEU often correlates poorly with human judgments. While BLEU only considers precision (how many n-grams in the machine translation appear in the reference), GLEU incorporates both precision and recall elements, making it more sensitive to translation errors. The metric works by calculating n-gram matches between the candidate translation and reference translations, but unlike BLEU, it also penalizes n-grams that appear in the candidate but not in any reference. This makes GLEU better at distinguishing between good and bad translations at the sentence level. GLEU scores range from 0 to 1, with higher scores indicating better translation quality. The metric was initially developed for evaluating grammatical error correction systems, where it's important to ensure that correct parts of text aren't unnecessarily changed while errors are fixed. It was later adopted for machine translation evaluation. Research has shown that GLEU correlates better with human judgments than BLEU, particularly for short segments and individual sentences. It's especially useful in the context of interactive machine translation or when evaluating improvements to translation systems at a fine-grained level. While GLEU hasn't replaced BLEU as the standard metric in most machine translation research, it's commonly used as a complementary measure, particularly when sentence-level evaluation is important. It represents an important step in the evolution of translation metrics toward better alignment with human perception of translation quality. | [HuggingFace: GLEU](https://huggingface.co/spaces/evaluate-metric/gleu), [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144) |
| chrF | chrF (символьный F-показатель) - это метрика оценки качества машинного перевода, которая работает на уровне символов, а не слов или подслов, как большинство других метрик. Она вычисляет F-показатель между символьными n-граммами машинного перевода и эталонного перевода. Метрика chrF особенно эффективна для морфологически богатых языков, где важны префиксы, суффиксы и флексии. Она показывает более высокую корреляцию с человеческими оценками, чем BLEU, особенно для средне и низкоресурсных языков. Имеется вариант chrF++, который также учитывает и словесные n-граммы, что еще больше повышает корреляцию с человеческими оценками. | chrF (character F-score) is a machine translation evaluation metric that operates at the character level rather than at the word or subword level like most other metrics. It was designed to address several limitations of word-based metrics, particularly for morphologically rich languages where prefixes, suffixes, and inflections carry significant meaning. The metric works by calculating the F-score (harmonic mean of precision and recall) of character n-grams between a candidate translation and reference translation. Typically, it considers character n-grams from length 1 to 6, giving more weight to longer n-grams. While metrics like BLEU or METEOR treat words as indivisible units, chrF recognizes partial word matches, which is especially valuable when evaluating translations into languages with complex word formation processes. For example, if a translation gets most of a long word correct but has a small error in a suffix, word-based metrics would count this as a complete error, whereas chrF would recognize the partial correctness. Research has shown that chrF correlates better with human judgments than BLEU, particularly for morphologically rich languages such as Czech, German, and Finnish. It's also particularly effective for low-resource language pairs, where translation systems often struggle with producing exact word matches. An extended version, chrF++, incorporates word n-grams alongside character n-grams, further improving correlation with human evaluations while maintaining the benefits of character-level assessment. The metric is language-independent and doesn't require any linguistic resources like stemmers or synonym dictionaries, making it applicable to virtually any language pair. It produces scores ranging from 0 to 100, with higher scores indicating better translation quality. Due to its simplicity, effectiveness, and language-independence, chrF has become a standard metric reported in machine translation research, especially when working with morphologically complex languages or low-resource scenarios. It's often used alongside word-based metrics like BLEU to provide a more comprehensive assessment of translation quality. | [HuggingFace: chrF](https://huggingface.co/spaces/evaluate-metric/chrf), [chrF: character n-gram F-score for automatic MT evaluation](https://aclanthology.org/W15-3049/) |
| Perplexity | Перплексия - это мера того, насколько хорошо вероятностная модель предсказывает выборку данных. В контексте обработки естественного языка она измеряет, насколько хорошо языковая модель предсказывает следующее слово или символ в последовательности. Чем ниже перплексия, тем лучше модель предсказывает данные. Математически перплексия является экспонентой от кросс-энтропии и может интерпретироваться как среднее количество равновероятных исходов при предсказании. Например, перплексия 10 означает, что модель так же "удивлена" наблюдением, как если бы она выбирала случайно из 10 равновероятных вариантов. Перплексия широко используется для оценки языковых моделей в различных задачах, таких как машинный перевод, распознавание речи и генерация текста. | Perplexity is a measurement of how well a probability model predicts a sample. In the context of natural language processing, it measures how well a language model can predict the next word or character in a sequence. Lower perplexity indicates that the model is better at predicting the data. Mathematically, perplexity is the exponentiation of the cross-entropy and can be interpreted as the weighted average number of choices a model needs to make when predicting. For instance, a perplexity of 10 indicates that the model is as surprised by the observation as if it were choosing uniformly among 10 options. Perplexity has been a standard metric for evaluating language models since the early days of statistical language modeling. It is often used as an intrinsic evaluation metric, meaning it directly evaluates the model's ability to model language without considering any specific downstream task. This makes it useful for comparing different language models on equal footing. However, it's important to note that perplexity doesn't always correlate with performance on downstream tasks or human judgments of text quality. A model with lower perplexity might not necessarily generate more coherent or contextually appropriate text than a model with higher perplexity. Additionally, perplexity can only be meaningfully compared between models with the same vocabulary, as different tokenization schemes can lead to different perplexity ranges. Despite these limitations, perplexity remains a valuable and widely used metric in language modeling, providing a quantitative measure of a model's predictive capabilities. | [HuggingFace: Perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity), [Wikipedia: Perplexity](https://en.wikipedia.org/wiki/Perplexity) |
| WIL | Word Information Lost (WIL, потерянная словесная информация) - это метрика для оценки качества распознавания речи, которая объединяет чувствительность и точность в одну меру. WIL количественно определяет долю информации, которая теряется из-за ошибок распознавания, включая как пропущенные слова (ошибки пропуска), так и ошибочно распознанные слова (ошибки вставки). Значения WIL варьируются от 0 до 1, где 0 означает идеальное распознавание (нет потери информации), а 1 указывает на полную потерю информации. Метрика WIL более устойчива к различиям в размере словаря и длине фраз, чем традиционные метрики, такие как WER. | Word Information Lost (WIL) is a metric for evaluating speech recognition quality that combines sensitivity and specificity into a single measure. WIL quantifies the proportion of information that is lost due to recognition errors, including both missed words (deletion errors) and incorrectly recognized words (insertion errors). Unlike Word Error Rate (WER), which simply counts the number of word errors, WIL considers the informational value of words and how errors affect the overall meaning. The metric is calculated using mutual information concepts from information theory, treating the recognition process as information transmission between the reference transcript and the system output. WIL values range from 0 to 1, where 0 indicates perfect recognition (no information loss) and 1 indicates complete information loss. One key advantage of WIL over traditional metrics like WER is its robustness to differences in vocabulary size and utterance length. This makes WIL particularly useful for comparing recognition performance across different languages, domains, or speech conditions. Additionally, WIL correlates well with human judgments of transcription quality, as it better captures the practical impact of recognition errors on understanding the spoken content. The metric is increasingly being adopted in speech recognition research, especially in scenarios where understanding the severity of recognition errors in terms of information preservation is more important than simply counting word substitutions, insertions, and deletions. | [HuggingFace: WIL](https://huggingface.co/spaces/evaluate-metric/wil), [Word Information Lost: A Comprehensive Metric for Speech Recognition Systems](https://www.isca-speech.org/archive/pdfs/interspeech_2016/morris16_interspeech.pdf) |
| NIST | NIST (National Institute of Standards and Technology) - это метрика оценки качества машинного перевода, представляющая собой модификацию BLEU. Основное отличие от BLEU заключается в том, что NIST придает больший вес менее частым n-граммам, которые считаются более информативными. NIST также использует арифметическое среднее вместо геометрического при вычислении оценки совпадения n-грамм, и более грамотно штрафует слишком короткие переводы. В результате NIST лучше различает качественные различия между системами перевода и меньше подвержен влиянию небольших изменений длины перевода по сравнению с BLEU. | NIST (National Institute of Standards and Technology) is a machine translation evaluation metric that represents a modification of the BLEU metric. The main difference is that NIST gives greater weight to less frequent n-grams, which are considered more informative, whereas BLEU treats all n-grams equally. NIST also uses arithmetic mean instead of geometric mean when calculating the score of n-gram matches, and applies a more sophisticated brevity penalty for translations that are too short. As a result, NIST better distinguishes qualitative differences between translation systems and is less affected by small changes in translation length compared to BLEU. The metric was developed by NIST (the National Institute of Standards and Technology) to address some of BLEU's limitations and has been used in various MT evaluation campaigns. While BLEU has remained more widely adopted in recent research, NIST continues to be used as a complementary metric that provides additional insights into translation quality, particularly when information content and distinctive phrases are important considerations. | [NIST Machine Translation Evaluation](https://www.nist.gov/itl/iad/mig/metrics-machine-translation-evaluation), [The NIST Metric](https://aclanthology.org/W05-0909.pdf) |
| METEOR | METEOR (Metric for Evaluation of Translation with Explicit ORdering, Метрика оценки перевода с явным упорядочиванием) - это метрика для оценки качества машинного перевода, которая была разработана для решения некоторых ограничений метрики BLEU. В отличие от BLEU, которая рассматривает только точное совпадение н-грамм, METEOR учитывает также синонимы, основы слов (стемминг) и парафразы. Это позволяет лучше оценивать семантическую эквивалентность переводов, а не только их лексическое сходство. Кроме того, METEOR учитывает как точность, так и полноту (прецизию и полноту), что дает более сбалансированную оценку. Исследования показывают, что METEOR лучше коррелирует с человеческими оценками качества перевода, чем BLEU, особенно на уровне отдельных предложений. | METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a machine translation evaluation metric designed to address several limitations of BLEU, the most widely used metric in the field. While BLEU only considers exact n-gram matches between a candidate translation and reference translations, METEOR incorporates additional linguistic features to better assess semantic equivalence. The metric works by first creating an alignment between the words in the candidate and reference translations. Unlike BLEU, which only considers precision, METEOR calculates both precision and recall, combining them into an F-measure with recall weighted higher than precision. This addresses BLEU's tendency to favor short translations. A key innovation of METEOR is its use of various types of word matching beyond exact matches: It considers stemming (matching word variants with the same root), synonymy (using WordNet or other resources to identify meaning-equivalent words), and paraphrase tables (recognizing longer phrasal equivalents). After computing the F-measure, METEOR applies a fragmentation penalty that addresses word order differences. This penalizes translations where matches are scattered rather than forming contiguous sequences, capturing fluency aspects that BLEU misses. METEOR produces scores from 0 to 1, with higher scores indicating better translation quality. The metric has evolved through several versions (METEOR, METEOR-NEXT, METEOR-Universal), each improving language coverage and alignment techniques. Research has consistently shown that METEOR correlates better with human judgments than BLEU, particularly at the sentence level where BLEU performs poorly. This makes it especially valuable for system development and fine-grained evaluation. METEOR requires more computational resources than BLEU and depends on language-specific resources for optimal performance, though more recent versions have reduced these dependencies. Despite these limitations, METEOR has become a standard metric reported alongside BLEU in machine translation research, particularly when semantic adequacy rather than just lexical similarity is important. | [HuggingFace: METEOR](https://huggingface.co/spaces/evaluate-metric/meteor), [METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments](https://aclanthology.org/W07-0734/) |
| BERTScore | BERTScore - это метрика оценки текста, которая использует предобученные контекстуальные представления слов из моделей BERT для сравнения генерируемых и референтных текстов. В отличие от традиционных метрик, таких как BLEU или ROUGE, которые основаны на лексических совпадениях, BERTScore оценивает семантическое сходство текстов. Это позволяет учитывать синонимы и семантически близкие фразы. BERTScore рассчитывает точность, полноту и F1-меру, сравнивая косинусные расстояния между векторными представлениями слов. | BERTScore is an evaluation metric for text generation that uses pre-trained contextual embeddings from BERT models to compute similarity scores between candidate and reference texts. Unlike traditional n-gram based metrics such as BLEU or ROUGE that rely on exact lexical matches, BERTScore captures semantic similarity between texts, allowing it to recognize synonyms, paraphrases, and semantically equivalent expressions. The metric works by computing token-level matching between sentences using cosine similarity between token embeddings from BERT. For each token in the candidate sentence, BERTScore finds the most similar token in the reference sentence based on their contextual embeddings. This approach better captures the semantic meaning of words in context rather than just surface forms. BERTScore calculates precision (how many tokens in the candidate have good matches in the reference), recall (how many tokens in the reference have good matches in the candidate), and F1 score (harmonic mean of precision and recall). Research has shown that BERTScore correlates substantially better with human judgments than lexical overlap metrics, particularly for tasks like machine translation, text summarization, and image captioning. The metric has several variants depending on which pre-trained model is used (e.g., BERT, RoBERTa, etc.) and whether importance weighting is applied to tokens. BERTScore addresses many limitations of traditional metrics by capturing semantic equivalence, properly handling word reordering, and being more robust to paraphrasing. It has become widely adopted in NLP research for evaluating text generation systems where semantic similarity is important. | [HuggingFace: BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore), [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675) |
| CIDEr | CIDEr (Consensus-based Image Description Evaluation, Оценка описания изображений на основе консенсуса) - это метрика, разработанная специально для оценки качества описаний изображений. Она измеряет сходство между сгенерированным описанием и набором эталонных описаний, созданных людьми. CIDEr учитывает консенсус между различными человеческими аннотациями, придавая больший вес словам и фразам, которые чаще встречаются в эталонных описаниях конкретного изображения, но реже встречаются в описаниях других изображений. | CIDEr (Consensus-based Image Description Evaluation) is a metric specifically designed for evaluating the quality of image descriptions or captions. Unlike generic text similarity metrics, CIDEr was developed to capture the nuances of how humans describe visual content. The metric measures the similarity between a generated caption and a set of reference captions created by humans, emphasizing consensus among different human annotations. CIDEr works by representing captions using Term Frequency-Inverse Document Frequency (TF-IDF) weighting, which assigns higher weights to words and phrases that frequently appear in references for a specific image but rarely appear in descriptions of other images. This captures the idea that good captions should mention distinctive aspects of an image rather than generic content. The metric computes cosine similarity between the TF-IDF vectors of the candidate caption and reference captions, typically using n-grams of various lengths (1-4 words) to capture both individual words and phrases. These n-gram similarities are averaged to produce the final CIDEr score. CIDEr values typically range from 0 to 10 (or higher), with higher values indicating better alignment with human consensus. A variant called CIDEr-D includes additional modifications to prevent gaming the metric, such as clipping n-gram counts and applying a length penalty. Research has shown that CIDEr correlates well with human judgments of caption quality and captures semantic relevance better than metrics like BLEU or ROUGE for image captioning tasks. The metric has become a standard evaluation measure in image captioning research and is widely used in related vision-language tasks. | [HuggingFace: CIDEr](https://huggingface.co/spaces/evaluate-metric/cider), [CIDEr: Consensus-based Image Description Evaluation](https://arxiv.org/abs/1411.5726) |
| SPICE | SPICE (Semantic Propositional Image Caption Evaluation, Семантическая пропозициональная оценка описаний изображений) - это метрика для оценки качества описаний изображений, которая, в отличие от метрик на основе n-грамм, фокусируется на семантическом содержании. SPICE работает путем преобразования описаний изображений в графы сцен (семантические графы), которые представляют объекты, атрибуты и отношения, упомянутые в описании. Затем она вычисляет F1-меру на основе совпадения элементов между графами сгенерированного и эталонного описаний. Этот подход позволяет лучше оценивать семантическую точность описаний, а не просто лексическое сходство. | SPICE (Semantic Propositional Image Caption Evaluation) is a metric for evaluating the quality of image descriptions that, unlike n-gram based metrics, focuses on semantic content rather than superficial text similarity. SPICE works by parsing image captions into scene graphs (semantic graphs) that represent the objects, attributes, and relationships mentioned in the description. It then calculates an F-score based on the matching elements between the candidate and reference caption graphs. This approach allows SPICE to better assess semantic accuracy of captions rather than just lexical similarity. For example, if a reference caption states "a man riding a bike" and a candidate caption says "a person on a bicycle," n-gram based metrics might give a low score due to different wording, but SPICE would recognize the semantic similarity. The metric breaks down evaluation into several semantic categories (objects, attributes, relations), providing more interpretable scores that align with human judgments of caption quality. Research has shown that SPICE correlates better with human evaluations on semantic correctness than metrics like BLEU, ROUGE, or CIDEr. However, since SPICE focuses exclusively on semantic content, it doesn't assess fluency or grammatical correctness. For this reason, it's often used in conjunction with other metrics like CIDEr to provide a more comprehensive evaluation of image caption quality. SPICE requires more computational resources than n-gram based metrics as it involves dependency parsing and semantic graph matching. Despite this limitation, it has become a standard evaluation metric in image captioning research, particularly when semantic accuracy is a primary concern. | [SPICE: Semantic Propositional Image Caption Evaluation](https://arxiv.org/abs/1607.08822), [GitHub: SPICE Implementation](https://github.com/peteanderson80/SPICE) |
| SPICE | SPICE (Semantic Propositional Image Caption Evaluation, Семантическая пропозициональная оценка описаний изображений) - это метрика для оценки качества описаний изображений, которая использует семантический анализ вместо простого сравнения н-грамм. SPICE преобразует описания изображений в графы семантических сцен, которые представляют объекты, атрибуты и отношения между ними. Затем сравнивается совпадение элементов этих графов, что позволяет оценить смысловое содержание описаний, а не просто текстовое сходство. | SPICE (Semantic Propositional Image Caption Evaluation) is a metric designed to evaluate the quality of image captions by analyzing their semantic content rather than just lexical similarity. Unlike traditional metrics like BLEU, ROUGE, or CIDEr that primarily compare n-gram overlap, SPICE focuses on how well the generated caption captures the meaning and semantic elements of the reference captions. The metric works by parsing both candidate and reference captions into scene graphs—structured representations that capture objects, attributes, and relationships mentioned in the text. For example, "a smiling woman holding a cat" would be parsed into a graph with nodes for "woman" and "cat," attributes like "smiling" for the woman, and a relationship "holding" between the woman and cat. SPICE then computes F-scores based on the overlap of these semantic propositions, measuring how well the candidate caption captures the same semantic content as the reference captions. This approach enables SPICE to recognize when captions express the same meaning using different words or syntactic structures, which n-gram based metrics often fail to detect. The metric also supports evaluation of specific semantic categories (objects, attributes, relations, etc.), allowing for detailed analysis of a caption system's specific strengths and weaknesses. Research has shown that SPICE correlates better with human judgments of semantic correctness than most other caption metrics. However, it does have limitations, including the fact that it doesn't evaluate fluency or grammatical correctness, and performance depends on the quality of the underlying scene graph parser. Because SPICE analyzes the actual meaning of captions rather than just their surface form, it has become an important complement to other metrics in image captioning research, often used alongside CIDEr to provide a more comprehensive evaluation of caption quality. | [HuggingFace: SPICE](https://huggingface.co/spaces/evaluate-metric/spice), [SPICE: Semantic Propositional Image Caption Evaluation](https://arxiv.org/abs/1607.08822) |
| DELTA | DELTA (Discriminative Evaluation of Language with Text Alignments, Дискриминативная оценка языка с текстовыми выравниваниями) - это метрика для оценки качества генерации текста, которая использует выравнивания текста и дискриминативное обучение. В отличие от многих других метрик, DELTA не требует референсных (эталонных) текстов и может оценивать согласованность, грамматику и смысловое соответствие сгенерированного текста. Метрика работает путем обучения дискриминативной модели на наборе пар текстов, состоящих из высококачественных и низкокачественных примеров, а затем использует эту модель для оценки новых текстов. DELTA показывает высокую корреляцию с человеческими оценками качества текста и может применяться к различным задачам генерации текста. | DELTA (Discriminative Evaluation of Language with Text Alignments) is a metric for evaluating text generation quality that uses text alignments and discriminative training. Unlike many other metrics, DELTA does not require reference texts and can evaluate the coherence, grammar, and semantic correspondence of generated text. The DELTA framework consists of a text encoder that produces embeddings for both input and output texts, and a classifier that makes quality judgments based on these embeddings. The metric is trained on a dataset of text pairs consisting of high-quality and low-quality examples, learning to distinguish between good and poor generations. This approach allows DELTA to capture various aspects of text quality, including coherence, fluency, and relevance to the given context or prompt. One key advantage of DELTA is its reference-free nature, making it particularly valuable for open-ended generation tasks where multiple diverse outputs might be acceptable. The metric can be applied to various text generation tasks, including summarization, dialogue generation, and creative writing. Research has shown that DELTA achieves high correlation with human judgments across different domains and languages. Unlike traditional n-gram based metrics that focus on surface-level similarities, DELTA can capture deeper semantic relationships and stylistic elements that contribute to perceived text quality. The metric can also be fine-tuned for specific domains or quality dimensions, providing flexibility for different evaluation needs. | [HuggingFace: DELTA](https://huggingface.co/spaces/evaluate-metric/delta), [DELTA: Discriminative Evaluation of Language with Text Alignments](https://arxiv.org/abs/2305.11746) |
| CUBE Score | CUBE Score (Contextualized Understanding-Based Evaluation, Оценка на основе контекстуализированного понимания) - это метрика для оценки качества генерации текста, основанная на моделях контекстуализированного понимания. CUBE Score оценивает, насколько хорошо генерируемый текст соответствует заданному контексту и запросу. Метрика использует предобученные языковые модели для создания векторных представлений текста и измерения их семантического соответствия. Она предназначена для более точной оценки содержательности, связности и релевантности генерируемого текста, чем традиционные метрики, основанные на лексическом совпадении. | CUBE Score (Contextualized Understanding-Based Evaluation) is a metric for evaluating text generation quality based on contextualized understanding models. It assesses how well generated text aligns with a given context and prompt. The metric leverages pre-trained language models to create vector representations of text and measure their semantic correspondence. CUBE Score is designed to provide a more accurate evaluation of the meaningfulness, coherence, and relevance of generated text than traditional lexical overlap metrics. The framework consists of components that evaluate different aspects of text quality, including semantic similarity, contextual relevance, and coherence. The metric has shown strong correlation with human judgments across various text generation tasks, including summarization, question answering, and dialogue generation. Unlike metrics that require exact matches or reference texts, CUBE Score can capture deeper semantic relationships and better handle paraphrasing and diverse expression styles. | [HuggingFace: CUBE Score](https://huggingface.co/spaces/evaluate-metric/cubescore), [CUBE Score: Contextualized Understanding-Based Evaluation for Text Generation](https://arxiv.org/abs/2201.03301) |
| SacreBLEU | SacreBLEU (Священный BLEU) - это стандартизированная реализация метрики BLEU для оценки качества машинного перевода. Она была разработана для решения проблемы несогласованности результатов BLEU в разных исследованиях из-за различий в предварительной обработке текста. SacreBLEU стандартизирует процесс токенизации, нормализации и вычисления метрики, что позволяет более достоверно сравнивать результаты разных моделей перевода. | SacreBLEU is a standardized implementation of the BLEU metric for evaluating machine translation quality. It was developed to address inconsistencies in BLEU scores across different research papers due to variations in tokenization and other preprocessing steps. The name "Sacre" (sacred) reflects its aim to provide a "blessed" or canonical implementation that researchers can use to ensure comparability of results. At its core, SacreBLEU performs the same calculation as traditional BLEU - comparing n-gram overlap between candidate translations and reference translations. However, it standardizes the entire evaluation pipeline, including tokenization, normalization, and score calculation. One key feature is that SacreBLEU includes a signature with each score, documenting the exact configuration used (tokenization method, smoothing, etc.), making results fully reproducible. SacreBLEU supports multiple reference translations and various tokenization schemes, including language-specific tokenizers for more accurate evaluation across different languages. It also includes options for character-level n-grams and different weighting schemes. The tool has become the standard for reporting BLEU scores in machine translation research, as it allows for fair comparison between different systems and replicability of results. Additionally, SacreBLEU can compute other translation metrics like chrF and TER. SacreBLEU is particularly important for benchmarking MT systems, as small differences in preprocessing can lead to significant variations in BLEU scores, potentially leading to incorrect conclusions about model performance. The metric is available as both a command-line tool and a Python library, making it easy to integrate into various research workflows. | [HuggingFace: SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu), [SacreBLEU: A Library for Canonicalized BLEU](https://arxiv.org/abs/1804.08771) |
| chrF | chrF (character n-gram F-score, Символьная n-граммная F-мера) - это метрика для оценки качества машинного перевода, которая работает на уровне символов, а не слов. В отличие от BLEU и других метрик, основанных на словах, chrF рассчитывает F-меру на основе совпадений символьных n-грамм между сгенерированным и эталонным переводами. Этот подход позволяет лучше учитывать морфологические изменения и частичные совпадения слов, что особенно полезно для морфологически богатых языков. | chrF (character n-gram F-score) is a metric for evaluating machine translation quality that operates at the character level rather than the word level. It was introduced to address some limitations of word-based metrics like BLEU, particularly for morphologically rich languages where small variations in word forms can result in complete word mismatches despite the translations being semantically similar. The metric calculates an F-score based on character n-gram matches between the generated translation and the reference translation. It typically uses character n-grams of varying lengths (usually 1-6) and combines precision (what percentage of character n-grams in the candidate translation also appear in the reference) and recall (what percentage of character n-grams in the reference also appear in the candidate) into a single F-score. By operating at the character level, chrF can capture partial word matches, which is especially valuable for languages with complex morphology, agglutination, or compounding. For example, if a translation contains "houses" instead of the reference "house", a word-level metric would count this as a complete mismatch, while chrF would recognize the substantial character overlap. Various versions of the metric exist, including chrF++ which also incorporates word n-grams to balance character and word-level information. Research has shown that chrF correlates better with human judgments than BLEU for many language pairs, particularly for morphologically rich target languages. The metric is relatively simple to calculate, language-independent, and requires no linguistic tools like stemmers or part-of-speech taggers. It has become increasingly popular as a complementary metric to BLEU in machine translation evaluation, and is often included in standard MT evaluation suites. | [HuggingFace: chrF](https://huggingface.co/spaces/evaluate-metric/chrf), [chrF: character n-gram F-score for automatic MT evaluation](https://aclanthology.org/W15-3049/) |
| TER | TER (Translation Edit Rate, Коэффициент редактирования перевода) - это метрика для оценки качества машинного перевода, которая измеряет количество правок (вставок, удалений, замен и перемещений), необходимых для преобразования машинного перевода в эталонный перевод. Значение TER рассчитывается как отношение минимального количества необходимых правок к общему количеству слов в эталонном переводе. Чем ниже значение TER, тем ближе машинный перевод к эталонному, и, следовательно, тем выше его качество. Существует также расширенная версия HTER (Human-targeted TER), которая учитывает правки, внесенные человеком-редактором. | TER (Translation Edit Rate) is a metric for evaluating machine translation quality that measures the number of edits required to transform machine translation output into a reference translation. Unlike metrics that focus on n-gram matches, TER directly quantifies the effort needed to correct a translation, making it particularly relevant for practical applications where post-editing is common. The metric calculates the minimum number of edits (insertions, deletions, substitutions, and shifts) required to match the reference, divided by the number of words in the reference. A lower TER score indicates better translation quality, as fewer edits are needed. The edits considered by TER include: word insertions, word deletions, word substitutions, and phrasal shifts (moving sequences of words as a unit). This last category is particularly important as it captures reordering phenomena that are common across languages with different syntactic structures. An extension of TER called HTER (Human-targeted Translation Edit Rate) uses human-corrected versions of machine translations as references, providing a more realistic assessment of the actual post-editing effort required. TER correlates reasonably well with human judgments and provides an intuitive measure that's easily explainable to non-specialists. The metric has been widely adopted in machine translation evaluation campaigns and is particularly valued in professional translation workflows where post-editing efficiency is a key concern. One advantage of TER is that it produces scores that can be interpreted as the percentage of words that need to be corrected, making it more directly interpretable than some other metrics. However, a limitation is that it treats all edit operations equally, when in reality some edits may be more cognitively demanding or time-consuming than others. | [HuggingFace: TER](https://huggingface.co/spaces/evaluate-metric/ter), [Translation Edit Rate: A Better, More Intuitive Metric](https://aclanthology.org/2006.amta-papers.25/) |
| COMET | COMET (Crosslingual Optimized Metric for Evaluation of Translation, Кроссязыковая оптимизированная метрика для оценки перевода) - это нейросетевая метрика для оценки качества машинного перевода, которая использует предобученные модели для оценки переводов на основе контекстуализированных представлений. В отличие от традиционных метрик, основанных на лексических совпадениях, COMET учитывает семантические и контекстные особенности языка. Метрика обучается на данных о человеческих оценках качества перевода и показывает высокую корреляцию с человеческими суждениями. | COMET (Crosslingual Optimized Metric for Evaluation of Translation) is a neural metric for evaluating machine translation quality that leverages pre-trained cross-lingual language models to assess translations based on their contextual representations. Unlike traditional lexical overlap metrics such as BLEU, TER, or chrF, COMET captures semantic and contextual aspects of language that better align with human judgments of translation quality. The core architecture of COMET consists of an encoder-based neural model that takes three inputs: source text, machine translation output, and reference translation. The model processes these inputs to generate contextualized embeddings and then predicts a quality score that represents how good the translation is. COMET is trained on human judgment data from various machine translation evaluation campaigns, learning to associate patterns in translations with human quality assessments. This supervision allows it to capture subtle aspects of translation quality that are difficult to formalize with rule-based metrics. One key advantage of COMET is its ability to handle linguistic phenomena such as synonymy, paraphrasing, and word reordering in a more nuanced way than n-gram based metrics. It can recognize when a translation conveys the same meaning using different words or structures, which is particularly important for languages with flexible word order or rich morphology. COMET has consistently shown higher correlation with human judgments compared to traditional metrics across various language pairs and domains. There are several variants of COMET, including reference-based models (which compare machine translations to human references) and reference-free models (which can evaluate translations without requiring human reference translations, only using the source text). The metric continues to evolve, with newer versions incorporating advances in language modeling and multi-task learning to further improve performance. COMET has been widely adopted in machine translation research and development, and has been used in major evaluation campaigns such as the Conference on Machine Translation (WMT). | [HuggingFace: COMET](https://huggingface.co/spaces/evaluate-metric/comet), [COMET: A Neural Framework for MT Evaluation](https://arxiv.org/abs/2009.09025) |
| BLEURT | BLEURT (Bilingual Evaluation Understudy with Representations from Transformers, Двуязычная оценка с представлениями из трансформеров) - это нейросетевая метрика для оценки качества машинного перевода, разработанная Google Research. Она основана на предварительно обученных моделях BERT и предназначена для преодоления ограничений традиционных метрик, основанных на лексических совпадениях. BLEURT обучается на человеческих оценках переводов и использует многозадачное обучение для улучшения своей робастности и обобщающей способности. | BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) is a neural metric for evaluating machine translation quality developed by Google Research. It builds upon the success of BERT and other transformer-based language models to create a more nuanced evaluation metric that correlates better with human judgments than traditional lexical overlap metrics. The key innovation of BLEURT is its training methodology, which involves a multi-step process. First, the model is pre-trained using synthetic data generated with massive text perturbations, helping it learn to recognize various types of textual differences. It is then fine-tuned on human judgments of translation quality, allowing it to learn which differences matter more to human evaluators. BLEURT takes as input a candidate translation and a reference translation, and outputs a quality score that represents how close the candidate is to the reference in terms of meaning and fluency. Unlike traditional metrics like BLEU, which rely on exact matches or simple n-gram overlaps, BLEURT captures semantic similarities even when different words or phrasings are used. One significant advantage of BLEURT is its robustness to distribution shifts - it performs well even on text domains or language pairs that differ from its training data. This robustness comes from its pre-training strategy that exposes the model to a wide variety of text perturbations and linguistic phenomena. The metric has shown strong correlation with human judgments across different language pairs and has been adopted in major machine translation evaluation campaigns. BLEURT has also been extended beyond translation evaluation to other text generation tasks like summarization and question answering. Like other neural metrics, BLEURT represents a shift towards more contextual, semantic evaluation of text generation quality, moving beyond surface-level features to assess deeper aspects of meaning preservation and fluency. | [HuggingFace: BLEURT](https://huggingface.co/spaces/evaluate-metric/bleurt), [BLEURT: Learning Robust Metrics for Text Generation](https://arxiv.org/abs/2004.04696) |
| ROUGE | ROUGE (Recall-Oriented Understudy for Gisting Evaluation, Направленный на полноту заместитель для оценки аннотаций) - это набор метрик для оценки качества автоматически созданных сводок или переводов. ROUGE сравнивает автоматически созданный текст с эталонными текстами, созданными человеком. Существуют различные варианты ROUGE, включая ROUGE-N (сравнение n-грамм), ROUGE-L (использует наибольшую общую подпоследовательность) и ROUGE-S (использует пропускающие биграммы). | ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics designed to evaluate automatic summarization and, to a lesser extent, machine translation. It works by comparing an automatically produced summary or translation against a set of reference texts created by humans. The central idea behind ROUGE is to determine how much of the reference content appears in the candidate text, with a focus on recall (how much of the reference information is captured). There are several variants of ROUGE metrics: ROUGE-N measures the overlap of n-grams between the system and reference summaries. Common values for N are 1 and 2, capturing unigram and bigram matches respectively. ROUGE-L uses the concept of Longest Common Subsequence (LCS) to measure the longest sequence of words that appear in both the candidate and reference texts in the same order, allowing for other words to appear in between the matching words. ROUGE-S, or Skip-Bigram measure, counts the overlap of any pair of words in their sentence order, allowing for arbitrary gaps between them. ROUGE-SU additionally includes unigram matches to address some limitations of skip-bigrams alone. While initially developed for summarization evaluation, ROUGE has been adapted for machine translation evaluation as well, particularly in scenarios where semantic adequacy is more important than fluency. The metrics are relatively simple to understand and implement, making them popular for various text generation tasks. ROUGE is often reported alongside other metrics like BLEU because it emphasizes different aspects of the generated text - while BLEU focuses on precision (how accurate the generated text is), ROUGE emphasizes recall (how complete the generated text is compared to references). ROUGE scores range from 0 to 1, with higher values indicating better overlap with reference texts. When used for translation evaluation, ROUGE tends to reward translations that preserve more of the source content, though it may not fully capture aspects like grammaticality or idiomatic expression. | [HuggingFace: ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge), [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013/) |
| GLEU | GLEU (Google BLEU, Модификация BLEU от Google) - это метрика для оценки качества машинного перевода, разработанная Google как модификация метрики BLEU. GLEU рассчитывает n-граммные совпадения между машинным переводом и эталонными переводами, но в отличие от BLEU, она сравнивает n-граммы как с оригиналом, так и с эталонным переводом. GLEU лучше коррелирует с человеческими оценками качества перевода на уровне предложений, особенно для коротких текстов. | GLEU (Google BLEU) is a metric for evaluating machine translation quality developed by Google as a modification of the traditional BLEU metric. While BLEU focuses solely on precision (how many of the n-grams in the machine translation appear in the reference), GLEU incorporates both precision and recall elements, making it more balanced. GLEU was originally designed for sentence-level evaluation where traditional BLEU scores can be unreliable due to their corpus-level design. The metric works by calculating both n-grams that match between the candidate translation and the reference (similar to BLEU), but also penalizes n-grams that appear in the candidate but not in the reference. This creates a more nuanced evaluation that better correlates with human judgments, especially for shorter texts. GLEU scores range from 0 to 1, with higher scores indicating better translations. The metric has been particularly useful in the development and evaluation of neural machine translation systems where sentence-by-sentence evaluation is often needed. GLEU has been shown to have better correlation with human judgments at the sentence level compared to standard BLEU, making it valuable for tasks that require fine-grained evaluation of translation quality. Like other n-gram based metrics, GLEU has limitations in capturing semantic equivalence when different words are used to express the same meaning, but its balanced approach between precision and recall provides a more comprehensive assessment than precision-only metrics. Google has used this metric extensively in the development of their neural machine translation systems, and it has become a standard evaluation tool in the field of machine translation research. | [GLEU: Google-BLEU](https://en.wikipedia.org/wiki/BLEU), [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144) |
| BLEU | BLEU (Bilingual Evaluation Understudy, Двуязычный заместитель оценки) - это одна из наиболее широко используемых метрик для оценки качества машинного перевода. BLEU измеряет точность машинного перевода путем сравнения n-грамм машинного перевода с n-граммами в эталонных переводах. Финальный показатель BLEU - это среднее геометрическое показателей совпадений n-грамм разных порядков, обычно от 1 до 4. | BLEU (Bilingual Evaluation Understudy) is one of the most widely used metrics for evaluating machine translation quality. Introduced by Papineni et al. in 2002, BLEU revolutionized automatic evaluation of machine translation by providing a method that correlates reasonably well with human judgments while being quick and inexpensive to calculate. BLEU works by comparing n-grams of the candidate translation with n-grams of reference translation(s) and counting the number of matches. These matches are position-independent - the same words in a different order still count as matches. The score is computed as a geometric mean of n-gram precisions for different values of n (typically 1 to 4), combined with a brevity penalty that penalizes translations that are shorter than the reference. The final score ranges from 0 to 1, where 1 is a perfect match with the reference(s). BLEU is designed primarily as a corpus-level metric, meaning it tends to be more reliable when calculated over a large set of sentences rather than individual sentences. When used at the sentence level, scores can be volatile and less reliable. Despite its widespread adoption, BLEU has several limitations: it only measures the precision of n-gram matches without considering recall (how much of the reference information is captured); it doesn't account for synonyms or paraphrases; it doesn't consider linguistic features like syntax or semantics; and it can be misleading for languages with rich morphology or free word order. Nonetheless, BLEU remains a standard evaluation metric in machine translation research and development due to its simplicity, efficiency, and reasonable correlation with human judgments. It's often reported in academic papers and used in comparative evaluations of different translation systems. Over time, various modifications and improvements to BLEU have been proposed, such as BLEU+1, smooth-BLEU, and the previously mentioned GLEU, each addressing specific limitations of the original metric. | [HuggingFace: BLEU](https://huggingface.co/spaces/evaluate-metric/bleu), [BLEU: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040.pdf) |
